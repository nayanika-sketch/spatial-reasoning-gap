{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yyyyifan/VLQA\") #[\"relation_KR_NC\"]\n",
    "\n",
    "sample = dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra experiments: masking and adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean result code \n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor \n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "# 1. Model and Processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# 2. Define a Prompt and Inference by Hugging Face\n",
    "#prompt = \"\"\"\n",
    "#List all the main objects, entities, and notable visual elements in this image. \n",
    "#For each item, include a brief description of its appearance and position in the image.\n",
    "#Format your response as a bulleted list.\n",
    "#\"\"\"\n",
    "\n",
    "def get_prediction(image, prompt):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "# 3. Define Perturbation Functions\n",
    "def mask_image(image, mask_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Applies a rectangular mask to a random region of the image.\n",
    "    The masked region is set to black.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    _, h, w = image_tensor.shape\n",
    "    # Calculate dimensions for the mask region.\n",
    "    mask_h = int(h * mask_percentage)\n",
    "    mask_w = int(w * mask_percentage)\n",
    "    # Choose a random location to apply the mask.\n",
    "    top = random.randint(0, h - mask_h)\n",
    "    left = random.randint(0, w - mask_w)\n",
    "    # Apply mask by zeroing out the selected region.\n",
    "    image_tensor[:, top:top+mask_h, left:left+mask_w] = 0.0\n",
    "    masked_image = ToPILImage()(image_tensor)\n",
    "    return masked_image\n",
    "\n",
    "def add_noise_image(image, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Adds random Gaussian noise to the image.\n",
    "    noise_level controls the intensity of the noise.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    noise = torch.randn_like(image_tensor) * noise_level\n",
    "    noisy_image_tensor = image_tensor + noise\n",
    "    noisy_image_tensor = torch.clamp(noisy_image_tensor, 0, 1)\n",
    "    noisy_image = ToPILImage()(noisy_image_tensor)\n",
    "    return noisy_image\n",
    "\n",
    "# 4. Define a Function to Compare Predictions\n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    Computes a similarity ratio between two texts.\n",
    "    Uses difflib's SequenceMatcher to obtain a ratio.\n",
    "    \"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6] \n",
    "noise_levels = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]      \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(1):\n",
    "    context = \"The entities in the image are arranged in a 3x3 grid and connected by arrows.\\n\"\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "    question = sample[\"question\"]\n",
    "    prompt = f\"{context}Question: {question}\\nAnswer: \"\n",
    "    \n",
    "    baseline_pred = get_prediction(image, prompt)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "    print(\"Prompt:\")\n",
    "    print(question)\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "    for p in mask_percentages:\n",
    "        perturbed_image = mask_image(image, mask_percentage=p)\n",
    "        display_images.append(perturbed_image)\n",
    "        display_titles.append(f\"Mask {p:.1f}\")\n",
    "        perturbed_pred = get_prediction(perturbed_image, prompt)\n",
    "        mask_sim = similarity(baseline_pred, perturbed_pred)\n",
    "        results.append({\n",
    "            'image_index': idx,\n",
    "            'perturbation': 'mask',\n",
    "            'level': p,\n",
    "            'similarity': mask_sim,\n",
    "            'prediction': perturbed_pred\n",
    "        })\n",
    "\n",
    "    for n in noise_levels:\n",
    "        perturbed_image = add_noise_image(image, noise_level=n)\n",
    "        display_images.append(perturbed_image)\n",
    "        display_titles.append(f\"Noise {n:.2f}\")\n",
    "        perturbed_pred = get_prediction(perturbed_image, prompt)\n",
    "        noise_sim = similarity(baseline_pred, perturbed_pred)\n",
    "        results.append({\n",
    "            'image_index': idx,\n",
    "            'perturbation': 'noise',\n",
    "            'level': n,\n",
    "            'similarity': noise_sim,\n",
    "            'prediction': perturbed_pred\n",
    "        })\n",
    "\n",
    "    num_variants = len(display_images)\n",
    "    fig, axs = plt.subplots(1, num_variants, figsize=(5 * num_variants, 5))\n",
    "    if num_variants == 1:\n",
    "        axs = [axs]\n",
    "    for ax, img, title in zip(axs, display_images, display_titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(f\"Visual Perturbations for Image {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n--- Perturbation Analysis Results for Image {idx} ---\")\n",
    "    for res in results:\n",
    "        if res['image_index'] == idx:\n",
    "            print(f\"{res['perturbation'].capitalize()} at level {res['level']}: similarity = {res['similarity']:.2f}\")\n",
    "            print(res['prediction'])\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean result code \n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "# 1. Model and Processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# 2. Define a Prompt and Inference by Hugging Face\n",
    "#prompt = \"\"\"\n",
    "#List all the main objects, entities, and notable visual elements in this image. \n",
    "#For each item, include a brief description of its appearance and position in the image.\n",
    "#Format your response as a bulleted list.\n",
    "#\"\"\"\n",
    "\n",
    "def get_prediction(image, prompt):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "# 3. Define Perturbation Functions\n",
    "def mask_image(image, mask_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Applies a rectangular mask to a random region of the image.\n",
    "    The masked region is set to black.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    _, h, w = image_tensor.shape\n",
    "    # Calculate dimensions for the mask region.\n",
    "    mask_h = int(h * mask_percentage)\n",
    "    mask_w = int(w * mask_percentage)\n",
    "    # Choose a random location to apply the mask.\n",
    "    top = random.randint(0, h - mask_h)\n",
    "    left = random.randint(0, w - mask_w)\n",
    "    # Apply mask by zeroing out the selected region.\n",
    "    image_tensor[:, top:top+mask_h, left:left+mask_w] = 0.0\n",
    "    masked_image = ToPILImage()(image_tensor)\n",
    "    return masked_image\n",
    "\n",
    "def add_noise_image(image, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Adds random Gaussian noise to the image.\n",
    "    noise_level controls the intensity of the noise.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    noise = torch.randn_like(image_tensor) * noise_level\n",
    "    noisy_image_tensor = image_tensor + noise\n",
    "    noisy_image_tensor = torch.clamp(noisy_image_tensor, 0, 1)\n",
    "    noisy_image = ToPILImage()(noisy_image_tensor)\n",
    "    return noisy_image\n",
    "\n",
    "# 4. Define a Function to Compare Predictions\n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    Computes a similarity ratio between two texts.\n",
    "    Uses difflib's SequenceMatcher to obtain a ratio.\n",
    "    \"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6] \n",
    "noise_levels = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]      \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(100):\n",
    "    context = \"The entities in the image are arranged in a 3x3 grid and connected by arrows.\\n\"\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "    question = sample[\"question\"]\n",
    "    prompt = f\"{context}Question: {question}\\nAnswer: \"\n",
    "    \n",
    "    baseline_pred = get_prediction(image, prompt)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "    print(\"Prompt:\")\n",
    "    print(question)\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "    for p in mask_percentages:\n",
    "        perturbed_image = mask_image(image, mask_percentage=p)\n",
    "        display_images.append(perturbed_image)\n",
    "        display_titles.append(f\"Mask {p:.1f}\")\n",
    "        perturbed_pred = get_prediction(perturbed_image, prompt)\n",
    "        mask_sim = similarity(baseline_pred, perturbed_pred)\n",
    "        results.append({\n",
    "            'image_index': idx,\n",
    "            'perturbation': 'mask',\n",
    "            'level': p,\n",
    "            'similarity': mask_sim,\n",
    "            'prediction': perturbed_pred\n",
    "        })\n",
    "\n",
    "    for n in noise_levels:\n",
    "        perturbed_image = add_noise_image(image, noise_level=n)\n",
    "        display_images.append(perturbed_image)\n",
    "        display_titles.append(f\"Noise {n:.2f}\")\n",
    "        perturbed_pred = get_prediction(perturbed_image, prompt)\n",
    "        noise_sim = similarity(baseline_pred, perturbed_pred)\n",
    "        results.append({\n",
    "            'image_index': idx,\n",
    "            'perturbation': 'noise',\n",
    "            'level': n,\n",
    "            'similarity': noise_sim,\n",
    "            'prediction': perturbed_pred\n",
    "        })\n",
    "\n",
    "    num_variants = len(display_images)\n",
    "    fig, axs = plt.subplots(1, num_variants, figsize=(5 * num_variants, 5))\n",
    "    if num_variants == 1:\n",
    "        axs = [axs]\n",
    "    for ax, img, title in zip(axs, display_images, display_titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(f\"Visual Perturbations for Image {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n--- Perturbation Analysis Results for Image {idx} ---\")\n",
    "    for res in results:\n",
    "        if res['image_index'] == idx:\n",
    "            print(f\"{res['perturbation'].capitalize()} at level {res['level']}: similarity = {res['similarity']:.2f}\")\n",
    "            print(res['prediction'])\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "# 1. Model and Processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# 2. Define a Prompt and Inference by Hugging Face\n",
    "#prompt = \"\"\"\n",
    "#List all the main objects, entities, and notable visual elements in this image. \n",
    "#For each item, include a brief description of its appearance and position in the image.\n",
    "#Format your response as a bulleted list.\n",
    "#\"\"\"\n",
    "\n",
    "def get_prediction(image, prompt):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "# 3. Define Perturbation Functions\n",
    "def mask_image(image, mask_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Applies a rectangular mask to a random region of the image.\n",
    "    The masked region is set to black.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    _, h, w = image_tensor.shape\n",
    "    mask_h = int(h * mask_percentage)\n",
    "    mask_w = int(w * mask_percentage)\n",
    "    top = (h - mask_h) // 2 \n",
    "    left = w - mask_w  \n",
    "    image_tensor[:, top:top+mask_h, left:left+mask_w] = 0.0\n",
    "    masked_image = ToPILImage()(image_tensor)\n",
    "    return masked_image\n",
    "\n",
    "def add_noise_image(image, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Adds random Gaussian noise to the image.\n",
    "    noise_level controls the intensity of the noise.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    noise = torch.randn_like(image_tensor) * noise_level\n",
    "    noisy_image_tensor = image_tensor + noise\n",
    "    noisy_image_tensor = torch.clamp(noisy_image_tensor, 0, 1)\n",
    "    noisy_image = ToPILImage()(noisy_image_tensor)\n",
    "    return noisy_image\n",
    "\n",
    "# 4. Define a Function to Compare Predictions\n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    Computes a similarity ratio between two texts.\n",
    "    Uses difflib's SequenceMatcher to obtain a ratio.\n",
    "    \"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6] \n",
    "noise_levels = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]      \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(20):\n",
    "    context = \"The entities in the image are arranged in a 3x3 grid and connected by arrows.\\n\"\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "    question = \"List all the entities crayon is connected to\"\n",
    "    prompt = f\"{context}Question: {question}\\nAnswer: \"\n",
    "    \n",
    "    baseline_pred = get_prediction(image, prompt)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "    print(\"Prompt:\")\n",
    "    print(question)\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "    for p in mask_percentages:\n",
    "        perturbed_image = mask_image(image, mask_percentage=p)\n",
    "        display_images.append(perturbed_image)\n",
    "        display_titles.append(f\"Mask {p:.1f}\")\n",
    "        perturbed_pred = get_prediction(perturbed_image, prompt)\n",
    "        mask_sim = similarity(baseline_pred, perturbed_pred)\n",
    "        results.append({\n",
    "            'image_index': idx,\n",
    "            'perturbation': 'mask',\n",
    "            'level': p,\n",
    "            'similarity': mask_sim,\n",
    "            'prediction': perturbed_pred\n",
    "        })\n",
    "\n",
    "    for n in noise_levels:\n",
    "        perturbed_image = add_noise_image(image, noise_level=n)\n",
    "        display_images.append(perturbed_image)\n",
    "        display_titles.append(f\"Noise {n:.2f}\")\n",
    "        perturbed_pred = get_prediction(perturbed_image, prompt)\n",
    "        noise_sim = similarity(baseline_pred, perturbed_pred)\n",
    "        results.append({\n",
    "            'image_index': idx,\n",
    "            'perturbation': 'noise',\n",
    "            'level': n,\n",
    "            'similarity': noise_sim,\n",
    "            'prediction': perturbed_pred\n",
    "        })\n",
    "\n",
    "    num_variants = len(display_images)\n",
    "    fig, axs = plt.subplots(1, num_variants, figsize=(5 * num_variants, 5))\n",
    "    if num_variants == 1:\n",
    "        axs = [axs]\n",
    "    for ax, img, title in zip(axs, display_images, display_titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(f\"Visual Perturbations for Image {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n--- Perturbation Analysis Results for Image {idx} ---\")\n",
    "    for res in results:\n",
    "        if res['image_index'] == idx:\n",
    "            print(f\"{res['perturbation'].capitalize()} at level {res['level']}: similarity = {res['similarity']:.2f}\")\n",
    "            print(res['prediction'])\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load Model and Processor\n",
    "# --------------------------\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Functions for Inference and Hidden State Extraction\n",
    "# --------------------------\n",
    "def get_image_embedding(image, prompt):\n",
    "    \"\"\"\n",
    "    Processes the image and prompt and performs a forward pass.\n",
    "    \n",
    "    Returns:\n",
    "      - final_image_embedding: the last hidden state (final layer output)\n",
    "      - hidden_states: tuple of hidden states from all layers\n",
    "      - inputs: the processed inputs (for generation)\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    full_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[full_text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    hidden_states = outputs.hidden_states  # tuple of hidden states (one per layer)\n",
    "    final_image_embedding = hidden_states[-1]  # final layer output\n",
    "    return final_image_embedding, hidden_states, inputs\n",
    "\n",
    "def get_raw_prediction(image, prompt):\n",
    "    \"\"\"\n",
    "    Uses the model's generate method to produce raw token IDs, prints them,\n",
    "    and then decodes to human-readable text.\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    full_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    _, _ = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[full_text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    output = model.generate(**inputs, max_new_tokens=128, return_dict_in_generate=True)\n",
    "    token_ids = output.sequences[0]\n",
    "    print(\"Raw token IDs:\", token_ids.tolist())\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return decoded_text\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    Computes a similarity ratio between two texts using difflib.\n",
    "    \"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def inspect_image_tokens(image, prompt, token_start=1, token_end=100, layer_index=-1):\n",
    "    \"\"\"\n",
    "    Inspects the image embedding for a specified layer. Selects a slice of tokens\n",
    "    (from token_start to token_end) from the hidden state at layer_index.\n",
    "    \n",
    "    Prints and returns the unique rows (i.e. unique embedding vectors) of that slice.\n",
    "    \n",
    "    Parameters:\n",
    "      - token_start, token_end: specify the token indices (in the sequence dimension)\n",
    "        that are expected to correspond to image tokens.\n",
    "      - layer_index: which layer's hidden state to inspect (default: -1, the final layer).\n",
    "    \"\"\"\n",
    "    final_image_embedding, hidden_states, _ = get_image_embedding(image, prompt)\n",
    "    # Choose the hidden state from the specified layer:\n",
    "    chosen_layer = hidden_states[layer_index]  # shape: [1, seq_length, hidden_dim]\n",
    "    tokens_embedding = chosen_layer[0, token_start:token_end, :]\n",
    "    \n",
    "    # Print the extracted embedding values.\n",
    "    print(f\"Image token embeddings (layer {layer_index}, tokens {token_start} to {token_end}):\")\n",
    "    print(tokens_embedding)\n",
    "    \n",
    "    unique_tokens = torch.unique(tokens_embedding, dim=0)\n",
    "    print(f\"\\nUnique image token representations:\")\n",
    "    print(unique_tokens)\n",
    "    print(f\"Total unique tokens: {unique_tokens.shape[0]}\")\n",
    "    return tokens_embedding, unique_tokens\n",
    "\n",
    "# --------------------------\n",
    "# 3. Main Example Usage\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load an image. Replace with the appropriate path or source (or dataset sample).\n",
    "    \n",
    "    sample = dataset[\"test\"][0]\n",
    "    image = sample[\"image\"]\n",
    "    # Define a prompt with context.\n",
    "    context = \"The entities in the image are arranged in a 3x3 grid and connected by arrows.\\n\"\n",
    "    question = \"How many text labels are there in the left column of the diagram?\"\n",
    "    prompt = f\"{context}Question: {question}\\nAnswer: \"\n",
    "    \n",
    "    # Get and print the image embedding shape.\n",
    "    image_embedding, hidden_states, inputs = get_image_embedding(image, prompt)\n",
    "    print(\"Final image embedding shape:\", image_embedding.shape)\n",
    "    \n",
    "    # Get the raw generation and print human-readable text.\n",
    "    human_readable_text = get_raw_prediction(image, prompt)\n",
    "    print(\"\\nTranslated text from image embedding:\")\n",
    "    print(human_readable_text)\n",
    "    \n",
    "    # Inspect and print the image tokens' text embedding representations.\n",
    "    # You can adjust token_start, token_end, and layer_index as needed.\n",
    "    tokens_embedding, unique_tokens = inspect_image_tokens(image, prompt, token_start=1, token_end=100, layer_index=-2)\n",
    "    \n",
    "    # Display the input image.\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"test\"][0]\n",
    "    image = sample[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, sample in enumerate(dataset[\"test\"]):\n",
    "    print(f\"{idx}: {sample['question']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare to ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "# 1. Model and Processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256*28*28,\n",
    "    max_pixels=512*28*28\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Define Perturbation Functions\n",
    "def mask_image(image, mask_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Applies a rectangular mask to a random region of the image.\n",
    "    The masked region is set to black.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    _, h, w = image_tensor.shape\n",
    "    # Calculate dimensions for the mask region.\n",
    "    mask_h = int(h * mask_percentage)\n",
    "    mask_w = int(w * mask_percentage)\n",
    "    # Choose a random location to apply the mask.\n",
    "    top = random.randint(0, h - mask_h)\n",
    "    left = random.randint(0, w - mask_w)\n",
    "    # Apply mask by zeroing out the selected region.\n",
    "    image_tensor[:, top:top+mask_h, left:left+mask_w] = 0.0\n",
    "    masked_image = ToPILImage()(image_tensor)\n",
    "    return masked_image\n",
    "\n",
    "sample_image = dataset[\"test\"][1][\"image\"]\n",
    "\n",
    "masked = mask_image(sample_image, mask_percentage=0.2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(20, 5))\n",
    "\n",
    "\n",
    "# Show masked image.\n",
    "axs.imshow(masked)\n",
    "axs.set_title(\"Masked\")\n",
    "axs.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "# 1. Model and Processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256*28*28,\n",
    "    max_pixels=512*28*28\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Define Perturbation Functions\n",
    "def add_noise_image(image, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Adds random Gaussian noise to the image.\n",
    "    noise_level controls the intensity of the noise.\n",
    "    \"\"\"\n",
    "    image_tensor = ToTensor()(image)\n",
    "    noise = torch.randn_like(image_tensor) * noise_level\n",
    "    noisy_image_tensor = image_tensor + noise\n",
    "    noisy_image_tensor = torch.clamp(noisy_image_tensor, 0, 1)\n",
    "    noisy_image = ToPILImage()(noisy_image_tensor)\n",
    "    return noisy_image\n",
    "\n",
    "sample_image = dataset[\"test\"][1][\"image\"]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(20, 5))\n",
    "\n",
    "noisy = add_noise_image(sample_image, noise_level=0.1)\n",
    "\n",
    "\n",
    "# Show noisy image.\n",
    "axs.imshow(noisy)\n",
    "axs.set_title(\"Noisy\")\n",
    "axs.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
