{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Please list all the entities (icons) you see in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icons_rep_grid')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Please list all the entities (icons) you see in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icons_rep_grid')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"Please identify all unique entities (icons) in this image, \"\n",
    "    \"count how many times each one appears, and list them in the format:\\n\"\n",
    "    \"[Entity Name]: [Count]\\n\"\n",
    "    \"[Entity Name]: [Count]\\n\"\n",
    "    \"...\"\n",
    ")\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icons_same_rep_grid')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"Please identify the entity (icon) in this image, \"\n",
    "    \"count how many times the entity appears, and list them in the format:\\n\"\n",
    "    \"[Entity Name]: [Count]\\n\"\n",
    "    \"...\"\n",
    ")\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"Please identify all unique entities (icons) in this image, and their relative position to other entities\"\n",
    ")\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"Please identify all unique entities (icons) in this image, and their relative position to another entity\"\n",
    ")\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"Please identify all unique entities (icons) in this image, and their relative position to another entity\"\n",
    ")\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"Is there a toast or a bird in the image\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[6]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"Is there a toast in the image\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[6]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"Is there a bird in the image\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[6]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"Is there anything related to baseball in the image?\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[10]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"What is the position of the pencil relative to the baseball?\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[10]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# ─── setup ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# ─── helper to call the model ────────────────────────────────────────────────────\n",
    "\n",
    "def query_model(messages, max_new_tokens=128):\n",
    "    # messages: list of {\"role\":..., \"content\":[{\"type\":\"image\",...},{\"type\":\"text\",...}]}\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[messages[0]['content'][0]['image']],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    return processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "# ─── two‐step prompting functions ─────────────────────────────────────────────────\n",
    "\n",
    "def step1_list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Step 1: Free‐form ask for all entities.\n",
    "    The model “thinks” and outputs e.g.:\n",
    "      Entities: pencil, baseball, notebook, lamp\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Look at the image and list all distinct objects (entities) you see.  \\n\"\n",
    "        \"Just output a comma-separated list, like:\\n\"\n",
    "        \"Entities: pencil, baseball, notebook, lamp\"\n",
    "    )\n",
    "    return query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "\n",
    "def step2_relative_positions(img: Image.Image, entities: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Step 2: For every entity, ask its position relative to ONE\n",
    "    of the other entities (you choose a consistent ordering).\n",
    "    We instruct the model to produce a structured table.\n",
    "    \"\"\"\n",
    "    # build the instruction dynamically:\n",
    "    ent_pairs = []\n",
    "    for i, a in enumerate(entities):\n",
    "        # choose the next entity in the list (wrap around)\n",
    "        b = entities[(i+1) % len(entities)]\n",
    "        ent_pairs.append(f\"- What is the position of **{a}** relative to **{b}**?\")\n",
    "    prompt = (\n",
    "        \"For each question below, output exactly one line of the form:\\n\"\n",
    "        \"`a relative_to b: <answer>`\\n\\n\"\n",
    "        + \"\\n\".join(ent_pairs)\n",
    "    )\n",
    "    return query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "\n",
    "# ─── simple parser for the structured output ────────────────────────────────────\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_relative_output(text: str) -> dict[tuple[str,str], str]:\n",
    "    \"\"\"\n",
    "    Parses lines like:\n",
    "      pencil relative_to baseball: above and to the right\n",
    "    into a dict {('pencil','baseball'): 'above and to the right', ...}\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s+relative_to\\s+(\\w+)\\s*:\\s*(.+)$\", re.IGNORECASE)\n",
    "    results = {}\n",
    "    for line in text.splitlines():\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            a, b, ans = m.group(1).lower(), m.group(2).lower(), m.group(3).strip()\n",
    "            results[(a, b)] = ans\n",
    "    return results\n",
    "\n",
    "# ─── glue code: load image, run both steps ────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','.jpeg')))\n",
    "    if len(files) < 1:\n",
    "        print(\"No images found.\")\n",
    "        exit(1)\n",
    "\n",
    "    path = os.path.join(OUTPUT_DIR, files[0])\n",
    "    img  = Image.open(path).convert(\"RGBA\")\n",
    "    print(f\"=== {files[0]} ===\")\n",
    "\n",
    "    # STEP 1\n",
    "    out1 = step1_list_entities(img)\n",
    "    print(\"\\nSTEP 1 — raw entities output:\")\n",
    "    print(out1)\n",
    "\n",
    "    # extract the comma‐list\n",
    "    ent_text = out1.split(\":\",1)[-1]\n",
    "    entities = [e.strip().lower() for e in ent_text.split(\",\") if e.strip()]\n",
    "    print(\"\\nParsed entities:\", entities)\n",
    "\n",
    "    # STEP 2\n",
    "    out2 = step2_relative_positions(img, entities)\n",
    "    print(\"\\nSTEP 2 — raw relative‐positions output:\")\n",
    "    print(out2)\n",
    "\n",
    "    # PARSE\n",
    "    rels = parse_relative_output(out2)\n",
    "    print(\"\\nStructured relative positions:\")\n",
    "    for (a, b), ans in rels.items():\n",
    "        print(f\" - {a} ⟶ relative to {b}: {ans}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"Which of these icons do you see in the image: leaf, doctor, car, dog. Only list the ones present in the image?\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[11]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"List the icons in the image\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[20]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    prompt = \"Do you see a dog or a heart or a bus in the image\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "\n",
    "    if len(files) >= 5:\n",
    "        fname = files[22]  # 5th image (0-indexed)\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n",
    "    else:\n",
    "        print(\"There are fewer than 5 images in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "####first images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "###(2) arrows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_only_one_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Please list the single entity you see in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Please list all the entities you see in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')) and f.startswith(\"grid\"))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"Which of the following exist in the image? You may choose multiple options:\\n\"\n",
    "    \"- Horizontal line\\n\"\n",
    "    \"- Vertical line\\n\"\n",
    "    \"- Horizontal bidirectional arrow\\n\"\n",
    "    \"- Vertical bidirectional arrow\\n\"\n",
    "    \"- Arrow pointing left\\n\"\n",
    "    \"- Arrow pointing right\\n\"\n",
    "    \"- Arrow pointing upwards\\n\"\n",
    "    \"- Arrow pointing downwards\\n\\n\"\n",
    "    \"For each entity you identify, please also describe its color.\"\n",
    ")\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')) and f.startswith(\"grid\"))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"For each entity you identify, please also describe its direction.\"\n",
    ")\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')) and f.startswith(\"grid\"))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_only_one_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"There is one arrow in this image.\\n\"\n",
    "    \"What direction is it pointing? Choose one of the following:\\n\"\n",
    "    \"- Left\\n\"\n",
    "    \"- Right\\n\"\n",
    "    \"- Up\\n\"\n",
    "    \"- Down\\n\"\n",
    "    \"- Left and right (bidirectional)\\n\"\n",
    "    \"- Up and down (bidirectional)\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_only_one_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "    \"What direction is the entity pointing? \"\n",
    ")\n",
    "\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_only_one_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Please list the single entity you see in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. entity recognition with qwen on synthetically generated dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_only_one_final')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Please list the single entity you see in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','jpeg')))\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display  \n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = os.path.expanduser('~/arrow_dataset_only_one_final')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Asks the model to identify the direction of the single arrow in the image.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Identify the direction of the single arrow in this image. \"\n",
    "        \"Choose one of the eight directions: \"\n",
    "        \"up, down, left, right, up-left, up-right, down-left, or down-right.\"\n",
    "    )\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=32)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', 'jpeg'))\n",
    "    )\n",
    "    if not files:\n",
    "        print(\"No images found in\", OUTPUT_DIR)\n",
    "        exit(0)\n",
    "\n",
    "    for fname in files:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)\n",
    "        direction = list_entities(img)\n",
    "        print(\"Predicted direction:\", direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info  # Keep if needed\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Please list all the entities (icons) you see in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')))\n",
    "\n",
    "    if not files:\n",
    "        print(\"No images found in the output directory.\")\n",
    "    else:\n",
    "        for fname in files[:5]:  # Only first 5 images\n",
    "            path = os.path.join(OUTPUT_DIR, fname)\n",
    "            img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "            print(f\"\\n=== {fname} ===\")\n",
    "            display(img)\n",
    "\n",
    "            entities = list_entities(img)\n",
    "            print(\"Entities recognized:\")\n",
    "            print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from IPython.display import display\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icons_same_rep_grid')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    raise FileNotFoundError(f\"Directory not found: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees\n",
    "    (and how many times each appears).\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Please tell me which icon entity you see in this image \"\n",
    "        \"and how many times it appears.\"\n",
    "    )\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    return reply\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    )\n",
    "    if not files:\n",
    "        print(\"No images found in\", OUTPUT_DIR)\n",
    "        exit(0)\n",
    "\n",
    "    for fname in files[:5]:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)  \n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(\"Entities recognized:\")\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icons_rep_grid')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    raise FileNotFoundError(f\"Directory not found: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees\n",
    "    (and how many times each appears).\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Please tell me which (icons) entities you see in this image \"\n",
    "        \"and how many times it appears.\"\n",
    "    )\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    return reply\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    )\n",
    "    if not files:\n",
    "        print(\"No images found in\", OUTPUT_DIR)\n",
    "        exit(0)\n",
    "\n",
    "    for fname in files[:5]:\n",
    "        path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "        print(f\"\\n=== {fname} ===\")\n",
    "        display(img)  \n",
    "\n",
    "        entities = list_entities(img)\n",
    "        print(\"Entities recognized:\")\n",
    "        print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info  # Keep if needed\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Do you see food (icons) in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')))\n",
    "\n",
    "    if not files:\n",
    "        print(\"No images found in the output directory.\")\n",
    "    else:\n",
    "        for fname in files[:5]:  # Only first 5 images\n",
    "            path = os.path.join(OUTPUT_DIR, fname)\n",
    "            img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "            print(f\"\\n=== {fname} ===\")\n",
    "            display(img)\n",
    "\n",
    "            entities = list_entities(img)\n",
    "            print(\"Entities recognized:\")\n",
    "            print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info  # Keep if needed\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids_with_arrows')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"List all entity pairs connected by arrows in this image.\"\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')))\n",
    "\n",
    "    if not files:\n",
    "        print(\"No images found in the output directory.\")\n",
    "    else:\n",
    "        for fname in files:  # Only first 5 images\n",
    "            path = os.path.join(OUTPUT_DIR, fname)\n",
    "            img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "            print(f\"\\n=== {fname} ===\")\n",
    "            display(img)\n",
    "\n",
    "            entities = list_entities(img)\n",
    "            print(\"Entities recognized:\")\n",
    "            print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info  # Keep if needed\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids_with_arrows')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"At which entity does the arrow start and at which entitiy does it end.\"\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')))\n",
    "\n",
    "    if not files:\n",
    "        print(\"No images found in the output directory.\")\n",
    "    else:\n",
    "        for fname in files:  # Only first 5 images\n",
    "            path = os.path.join(OUTPUT_DIR, fname)\n",
    "            img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "            print(f\"\\n=== {fname} ===\")\n",
    "            display(img)\n",
    "\n",
    "            entities = list_entities(img)\n",
    "            print(\"Entities recognized:\")\n",
    "            print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os\n",
    "\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids_with_arrows')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "K          = 1000   # max number of patches to try\n",
    "\n",
    "\n",
    "files = sorted(f for f in os.listdir(OUTPUT_DIR) if f.endswith('.png'))\n",
    "if not files:\n",
    "    raise RuntimeError(f\"No images in {OUTPUT_DIR}\")\n",
    "\n",
    "img_path = os.path.join(OUTPUT_DIR, files[0])\n",
    "orig: Image.Image = Image.open(img_path).convert(\"RGB\")\n",
    "W, H = orig.size\n",
    "print(f\"Original image size: {W}×{H}\")\n",
    "\n",
    "patch_unit, merge_size = 14, 2\n",
    "P = patch_unit * merge_size       # e.g. 28×28 patches\n",
    "cols = W // P\n",
    "total = cols * (H // P)\n",
    "print(f\"Image can be split into {cols} columns and {H//P} rows → {total} patches of {P}×{P}\")\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def predict(img: Image.Image, prompt: str):\n",
    "    # wrap into the Qwen2-VL chat format\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # process vision info if needed by your util\n",
    "    _ , _ = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text], images=img, return_tensors=\"pt\", padding=True\n",
    "    ).to(DEVICE)\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    # strip off prompt tokens\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    \"Which entities (icons) are connected together by an arrow in this image?  \"\n",
    "    \"List each connection in the format:\\n\"\n",
    "    \"[entity A] connected to [entity B]\"\n",
    ")\n",
    "\n",
    "\n",
    "for k in range(1, min(total, K) + 1):\n",
    "    # how many mosaic rows we need\n",
    "    rows = math.ceil(k / cols)\n",
    "    w_mosaic = min(k, cols) * P\n",
    "    h_mosaic = rows * P\n",
    "\n",
    "    # build mosaic of the first k patches\n",
    "    mosaic = Image.new(\"RGB\", (w_mosaic, h_mosaic))\n",
    "    for idx in range(k):\n",
    "        r, c = divmod(idx, cols)\n",
    "        src_box = (c*P, r*P, c*P+P, r*P+P)\n",
    "        patch   = orig.crop(src_box)\n",
    "        dst     = ((idx % cols)*P, (idx // cols)*P)\n",
    "        mosaic.paste(patch, (dst[0], dst[1], dst[0]+P, dst[1]+P))\n",
    "\n",
    "    # display to visually verify\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(mosaic)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Mosaic of first {k} patches\")\n",
    "    plt.show()\n",
    "\n",
    "    # ask the model\n",
    "    out = predict(mosaic, prompt)\n",
    "    print(f\"Prediction with {k} patches:\\n{out}\\n\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os\n",
    "\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids_with_arrows')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "K          = 1000   # max number of patches per image\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def predict(img: Image.Image, prompt: str) -> str:\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    _ , _ = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text], images=img, return_tensors=\"pt\", padding=True\n",
    "    ).to(DEVICE)\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    \"Which entities (icons) are connected together by an arrow in this image?  \"\n",
    "    \"List each connection in the format:\\n\"\n",
    "    \"[entity A] connected to [entity B]\"\n",
    ")\n",
    "\n",
    "\n",
    "files = sorted(f for f in os.listdir(OUTPUT_DIR) if f.lower().endswith('.png'))\n",
    "if not files:\n",
    "    raise RuntimeError(f\"No images found in {OUTPUT_DIR}\")\n",
    "\n",
    "for img_name in files:\n",
    "    img_path = os.path.join(OUTPUT_DIR, img_name)\n",
    "    orig = Image.open(img_path).convert(\"RGB\")\n",
    "    W, H = orig.size\n",
    "    print(f\"\\n=== {img_name} ({W}×{H}) ===\")\n",
    "\n",
    "    patch_unit, merge_size = 14, 2\n",
    "    P = patch_unit * merge_size\n",
    "    cols = W // P\n",
    "    total = cols * (H // P)\n",
    "    print(f\"Split into {cols} columns × {H//P} rows = {total} patches of size {P}×{P}\")\n",
    "\n",
    "    for k in range(1, min(total, K) + 1):\n",
    "        rows = math.ceil(k / cols)\n",
    "        w_mos = min(k, cols) * P\n",
    "        h_mos = rows * P\n",
    "\n",
    "        mosaic = Image.new(\"RGB\", (w_mos, h_mos))\n",
    "        for idx in range(k):\n",
    "            r, c = divmod(idx, cols)\n",
    "            src = (c*P, r*P, c*P+P, r*P+P)\n",
    "            patch = orig.crop(src)\n",
    "            dst = ((idx % cols)*P, (idx // cols)*P)\n",
    "            mosaic.paste(patch, (dst[0], dst[1], dst[0]+P, dst[1]+P))\n",
    "\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(mosaic)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{img_name}: first {k} patches\")\n",
    "        plt.show()\n",
    "\n",
    "        out = predict(mosaic, prompt)\n",
    "        print(f\"[{k:03d}/{total:03d}] {out}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info  # Keep if needed\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def list_entities(img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Returns the model's text listing of all the icons/entities it sees.\n",
    "    \"\"\"\n",
    "    prompt = \"Do you see cat (icons) in this image.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[img],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    reply = processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png', '.jpg', 'jpeg')))\n",
    "\n",
    "    if not files:\n",
    "        print(\"No images found in the output directory.\")\n",
    "    else:\n",
    "        for fname in files[:5]:  # Only first 5 images\n",
    "            path = os.path.join(OUTPUT_DIR, fname)\n",
    "            img  = Image.open(path).convert(\"RGBA\")\n",
    "\n",
    "            print(f\"\\n=== {fname} ===\")\n",
    "            display(img)\n",
    "\n",
    "            entities = list_entities(img)\n",
    "            print(\"Entities recognized:\")\n",
    "            print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "# ─── setup ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# ─── helper ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def query_model(messages, max_new_tokens=128):\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[messages[0]['content'][0]['image']],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    return processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "# ─── STEP 1: list entities ─────────────────────────────────────────────────────────\n",
    "\n",
    "def step1_list_entities(img: Image.Image) -> list[str]:\n",
    "    display(img)  # always show image\n",
    "    prompt = (\n",
    "        \"Please look at the image and list *all* distinct objects you see, \"\n",
    "        \"in a single comma-separated line.  \"\n",
    "        \"For example:\\n\"\n",
    "        \"Entities: pencil, baseball, notebook, lamp\"\n",
    "    )\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "    # parse after the colon:\n",
    "    ent_text = raw.split(\":\", 1)[-1]\n",
    "    return [e.strip().lower() for e in ent_text.split(\",\") if e.strip()]\n",
    "\n",
    "# ─── STEP 2: ask relative positions ─────────────────────────────────────────────────\n",
    "\n",
    "def step2_relative_positions(img: Image.Image, entities: list[str]) -> dict[tuple[str,str], str]:\n",
    "    display(img)\n",
    "    # build Qs comparing each entity to the *first* entity\n",
    "    ref = entities[0]\n",
    "    questions = []\n",
    "    for e in entities[1:]:\n",
    "        questions.append(f\"- Where is **{e}** relative to **{ref}**? \"\n",
    "                         \"(choose one or combine: above, below, left, right, in front of, behind)\")\n",
    "    prompt = (\n",
    "        \"For each line below, answer in the exact format:\\n\"\n",
    "        \"`{entity} relative_to {reference}: <spatial relationship>`\\n\\n\"\n",
    "        + \"\\n\".join(questions)\n",
    "    )\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "\n",
    "    # parse into dict\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s+relative_to\\s+(\\w+)\\s*:\\s*(.+)$\", re.IGNORECASE)\n",
    "    rels = {}\n",
    "    for line in raw.splitlines():\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            a, b, ans = m.group(1).lower(), m.group(2).lower(), m.group(3).strip()\n",
    "            rels[(a,b)] = ans\n",
    "    return rels\n",
    "\n",
    "# ─── main ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
    "                   if f.lower().endswith(('.png','.jpg','.jpeg')))\n",
    "    if not files:\n",
    "        print(\"No images found.\")\n",
    "        exit(1)\n",
    "\n",
    "    path = os.path.join(OUTPUT_DIR, files[1])\n",
    "    img  = Image.open(path).convert(\"RGBA\")\n",
    "    print(f\"=== {files[0]} ===\\n\")\n",
    "\n",
    "    ents = step1_list_entities(img)\n",
    "    print(\"Parsed entities:\", ents, \"\\n\")\n",
    "\n",
    "    rels = step2_relative_positions(img, ents)\n",
    "    print(\"Relative positions:\")\n",
    "    for (a,b), txt in rels.items():\n",
    "        print(f\" - {a} relative to {b}: {txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from IPython.display import display\n",
    "\n",
    "# ─── Configuration ───────────────────────────────────────────────────────────────\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# ─── Model & Processor Setup ─────────────────────────────────────────────────────\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# ─── Helper to call the model ────────────────────────────────────────────────────\n",
    "\n",
    "def query_model(messages, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    messages: [\n",
    "      {\"role\":\"user\", \"content\":[{\"type\":\"image\",\"image\":img},\n",
    "                                  {\"type\":\"text\",\"text\":prompt} ]}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[messages[0]['content'][0]['image']],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    return processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "# ─── Step 1: List entities ───────────────────────────────────────────────────────\n",
    "\n",
    "def step1_list_entities(img: Image.Image) -> list[str]:\n",
    "    display(img)  # Always show the image\n",
    "    prompt = (\n",
    "        \"Please look at the image and list *all* distinct objects you see, \"\n",
    "        \"in a single comma-separated line.  \\n\\n\"\n",
    "        \"For example:\\n\"\n",
    "        \"Entities: pencil, baseball, notebook, lamp\"\n",
    "    )\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "    # Parse the comma-separated list after the colon\n",
    "    ent_text = raw.split(\":\", 1)[-1]\n",
    "    return [e.strip().lower() for e in ent_text.split(\",\") if e.strip()]\n",
    "\n",
    "# ─── Step 2: Chain‐of‐Thought + Structured Extraction ─────────────────────────────\n",
    "\n",
    "def step2_relative_positions_with_cot(img: Image.Image, entities: list[str]) -> dict[tuple[str,str], str]:\n",
    "    display(img)\n",
    "    ref = entities[0]\n",
    "    questions = [\n",
    "        f\"- Where is **{e}** relative to **{ref}**?\"\n",
    "        for e in entities[1:]\n",
    "    ]\n",
    "    prompt = \"\"\"\\\n",
    "Below, for each comparison, first *think step by step* about where the second object lies relative to the first.  \n",
    "Label that section **Reasoning:**.  \n",
    "Then, in a separate **Answer:** section, output *only* lines of the form\n",
    "\n",
    "    entity relative_to reference: <spatial phrase>\n",
    "\n",
    "Use exactly one or a combination of: “above”, “below”, “left”, “right”, “in front of”, “behind”.\n",
    "\n",
    "Here are the comparisons:\n",
    "%s\n",
    "\n",
    "---\n",
    "Make sure **Answer:** contains *only* your concise relative-position lines.\n",
    "\"\"\" % (\"\\n\".join(questions))\n",
    "\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }], max_new_tokens=256)\n",
    "\n",
    "    # Extract just the Answer: block\n",
    "    if \"Answer:\" in raw:\n",
    "        answer_block = raw.split(\"Answer:\", 1)[1]\n",
    "    else:\n",
    "        answer_block = raw\n",
    "\n",
    "    # Parse lines like \"chicken relative_to key: above and to the right\"\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s+relative_to\\s+(\\w+)\\s*:\\s*(.+)$\", re.IGNORECASE)\n",
    "    rels = {}\n",
    "    for line in answer_block.splitlines():\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            a, b, ans = m.group(1).lower(), m.group(2).lower(), m.group(3).strip()\n",
    "            rels[(a, b)] = ans\n",
    "    return rels\n",
    "\n",
    "# ─── Main Execution ──────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    )\n",
    "    if not files:\n",
    "        print(\"No images found.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Choose your image (e.g., the first one)\n",
    "    img_path = os.path.join(OUTPUT_DIR, files[0])\n",
    "    img = Image.open(img_path).convert(\"RGBA\")\n",
    "    print(f\"=== {files[0]} ===\\n\")\n",
    "\n",
    "    # Step 1: List entities\n",
    "    entities = step1_list_entities(img)\n",
    "    print(\"Parsed entities:\", entities, \"\\n\")\n",
    "\n",
    "    # Step 2: Get relative positions\n",
    "    rels = step2_relative_positions_with_cot(img, entities)\n",
    "    print(\"Relative positions:\")\n",
    "    for (a, b), txt in rels.items():\n",
    "        print(f\" - {a} relative to {b}: {txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from IPython.display import display\n",
    "\n",
    "# ─── Configuration ───────────────────────────────────────────────────────────────\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# ─── Model & Processor Setup ─────────────────────────────────────────────────────\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# ─── Helper to call the model ────────────────────────────────────────────────────\n",
    "\n",
    "def query_model(messages, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    messages: [\n",
    "      {\"role\":\"user\", \"content\":[{\"type\":\"image\",\"image\":img},\n",
    "                                  {\"type\":\"text\",\"text\":prompt} ]}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[messages[0]['content'][0]['image']],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    return processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "# ─── Step 1: List entities ───────────────────────────────────────────────────────\n",
    "\n",
    "def step1_list_entities(img: Image.Image) -> list[str]:\n",
    "    display(img)  # Always show the image\n",
    "    prompt = (\n",
    "        \"Please look at the image and list *all* distinct objects you see, \"\n",
    "        \"in a single comma-separated line.  \\n\\n\"\n",
    "        \"For example:\\n\"\n",
    "        \"Entities: pencil, baseball, notebook, lamp\"\n",
    "    )\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "    # Parse the comma-separated list after the colon\n",
    "    ent_text = raw.split(\":\", 1)[-1]\n",
    "    return [e.strip().lower() for e in ent_text.split(\",\") if e.strip()]\n",
    "\n",
    "# ─── Step 2: Chain‐of‐Thought + Structured Extraction ─────────────────────────────\n",
    "\n",
    "def step2_relative_positions_with_cot(img: Image.Image, entities: list[str]) -> dict[tuple[str,str], str]:\n",
    "    display(img)\n",
    "    ref = entities[0]\n",
    "    questions = [\n",
    "        f\"- Where is **{e}** relative to **{ref}**?\"\n",
    "        for e in entities[1:]\n",
    "    ]\n",
    "    prompt = \"\"\"\\\n",
    "Below, for each comparison, first *think step by step* about where the second object lies relative to the first.  \n",
    "Label that section **Reasoning:**.  \n",
    "Then, in a separate **Answer:** section, output *only* lines of the form\n",
    "\n",
    "    entity relative_to reference: <spatial phrase>\n",
    "\n",
    "Use exactly one or a combination of: “above”, “below”, “left”, “right”, “in front of”, “behind”.\n",
    "\n",
    "Here are the comparisons:\n",
    "%s\n",
    "\n",
    "---\n",
    "Make sure **Answer:** contains *only* your concise relative-position lines.\n",
    "\"\"\" % (\"\\n\".join(questions))\n",
    "\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }], max_new_tokens=256)\n",
    "\n",
    "    # Extract just the Answer: block\n",
    "    if \"Answer:\" in raw:\n",
    "        answer_block = raw.split(\"Answer:\", 1)[1]\n",
    "    else:\n",
    "        answer_block = raw\n",
    "\n",
    "    # Parse lines like \"chicken relative_to key: above and to the right\"\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s+relative_to\\s+(\\w+)\\s*:\\s*(.+)$\", re.IGNORECASE)\n",
    "    rels = {}\n",
    "    for line in answer_block.splitlines():\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            a, b, ans = m.group(1).lower(), m.group(2).lower(), m.group(3).strip()\n",
    "            rels[(a, b)] = ans\n",
    "    return rels\n",
    "\n",
    "# ─── Main Execution ──────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    )\n",
    "    if not files:\n",
    "        print(\"No images found.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Choose your image (e.g., the first one)\n",
    "    img_path = os.path.join(OUTPUT_DIR, files[5])\n",
    "    img = Image.open(img_path).convert(\"RGBA\")\n",
    "    print(f\"=== {files[0]} ===\\n\")\n",
    "\n",
    "    # Step 1: List entities\n",
    "    entities = step1_list_entities(img)\n",
    "    print(\"Parsed entities:\", entities, \"\\n\")\n",
    "\n",
    "    # Step 2: Get relative positions\n",
    "    rels = step2_relative_positions_with_cot(img, entities)\n",
    "    print(\"Relative positions:\")\n",
    "    for (a, b), txt in rels.items():\n",
    "        print(f\" - {a} relative to {b}: {txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from IPython.display import display\n",
    "\n",
    "# ─── Configuration ───────────────────────────────────────────────────────────────\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# ─── Model & Processor Setup ─────────────────────────────────────────────────────\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# ─── Helper to call the model ────────────────────────────────────────────────────\n",
    "\n",
    "def query_model(messages, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    messages: [\n",
    "      {\"role\":\"user\", \"content\":[{\"type\":\"image\",\"image\":img},\n",
    "                                  {\"type\":\"text\",\"text\":prompt} ]}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[messages[0]['content'][0]['image']],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    return processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "# ─── Step 1: List entities ───────────────────────────────────────────────────────\n",
    "\n",
    "def step1_list_entities(img: Image.Image) -> list[str]:\n",
    "    display(img)  # Always show the image\n",
    "    prompt = (\n",
    "        \"Please look at the image and list *all* distinct objects you see, \"\n",
    "        \"in a single comma-separated line.  \\n\\n\"\n",
    "        \"For example:\\n\"\n",
    "        \"Entities: pencil, baseball, notebook, lamp\"\n",
    "    )\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "    # Parse the comma-separated list after the colon\n",
    "    ent_text = raw.split(\":\", 1)[-1]\n",
    "    return [e.strip().lower() for e in ent_text.split(\",\") if e.strip()]\n",
    "\n",
    "# ─── Step 2: Chain‐of‐Thought + Structured Extraction ─────────────────────────────\n",
    "\n",
    "def step2_relative_positions_with_cot(img: Image.Image, entities: list[str]) -> dict[tuple[str,str], str]:\n",
    "    display(img)\n",
    "    ref = entities[0]\n",
    "    questions = [\n",
    "        f\"- Where is **{e}** relative to **{ref}**?\"\n",
    "        for e in entities[1:]\n",
    "    ]\n",
    "    prompt = \"\"\"\\\n",
    "Below, for each comparison, first *think step by step* about where the second object lies relative to the first.  \n",
    "Label that section **Reasoning:**.  \n",
    "Then, in a separate **Answer:** section, output *only* lines of the form\n",
    "\n",
    "    entity relative_to reference: <spatial phrase>\n",
    "\n",
    "Use exactly one or a combination of: “above”, “below”, “left”, “right”, “in front of”, “behind”.\n",
    "\n",
    "Here are the comparisons:\n",
    "%s\n",
    "\n",
    "---\n",
    "Make sure **Answer:** contains *only* your concise relative-position lines.\n",
    "\"\"\" % (\"\\n\".join(questions))\n",
    "\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }], max_new_tokens=256)\n",
    "\n",
    "    # Extract just the Answer: block\n",
    "    if \"Answer:\" in raw:\n",
    "        answer_block = raw.split(\"Answer:\", 1)[1]\n",
    "    else:\n",
    "        answer_block = raw\n",
    "\n",
    "    # Parse lines like \"chicken relative_to key: above and to the right\"\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s+relative_to\\s+(\\w+)\\s*:\\s*(.+)$\", re.IGNORECASE)\n",
    "    rels = {}\n",
    "    for line in answer_block.splitlines():\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            a, b, ans = m.group(1).lower(), m.group(2).lower(), m.group(3).strip()\n",
    "            rels[(a, b)] = ans\n",
    "    return rels\n",
    "\n",
    "# ─── Main Execution ──────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    )\n",
    "    if not files:\n",
    "        print(\"No images found.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Choose your image (e.g., the first one)\n",
    "    img_path = os.path.join(OUTPUT_DIR, files[2])\n",
    "    img = Image.open(img_path).convert(\"RGBA\")\n",
    "    print(f\"=== {files[0]} ===\\n\")\n",
    "\n",
    "    # Step 1: List entities\n",
    "    entities = step1_list_entities(img)\n",
    "    print(\"Parsed entities:\", entities, \"\\n\")\n",
    "\n",
    "    # Step 2: Get relative positions\n",
    "    rels = step2_relative_positions_with_cot(img, entities)\n",
    "    print(\"Relative positions:\")\n",
    "    for (a, b), txt in rels.items():\n",
    "        print(f\" - {a} relative to {b}: {txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "OUTPUT_DIR = os.path.expanduser('~/icon645/icon_grids')\n",
    "DEVICE     = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "def query_model(messages, max_new_tokens=128):\n",
    "    chat_input = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[chat_input],\n",
    "        images=[messages[0]['content'][0]['image']],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    gen = outputs[0][ inputs.input_ids.shape[-1]: ]\n",
    "    return processor.batch_decode(\n",
    "        [gen],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "\n",
    "def step1_list_entities(img: Image.Image) -> list[str]:\n",
    "    display(img)  # Always show the image\n",
    "    prompt = (\n",
    "        \"Please look at the image and list *all* distinct objects you see, \"\n",
    "        \"in a single comma-separated line.  \\n\\n\"\n",
    "        \"For example:\\n\"\n",
    "        \"Entities: pencil, baseball, notebook, lamp\"\n",
    "    )\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }])\n",
    "    ent_text = raw.split(\":\", 1)[-1]\n",
    "    return [e.strip().lower() for e in ent_text.split(\",\") if e.strip()]\n",
    "\n",
    "\n",
    "def step2_relative_positions_with_cot(img: Image.Image, entities: list[str]) -> dict[tuple[str,str], str]:\n",
    "    display(img)\n",
    "    ref = entities[0]\n",
    "    questions = [\n",
    "        f\"- Where is **{e}** relative to **{ref}**?\"\n",
    "        for e in entities[1:]\n",
    "    ]\n",
    "    prompt = \"\"\"\\\n",
    "Below, for each comparison, first *think step by step* about where the second object lies relative to the first.  \n",
    "Label that section **Reasoning:**.  \n",
    "Then, in a separate **Answer:** section, output *only* lines of the form\n",
    "\n",
    "    entity relative_to reference: <spatial phrase>\n",
    "\n",
    "Use exactly one or a combination of: “up”, “down”, “left”, “right”.\n",
    "\n",
    "Here are the comparisons:\n",
    "%s\n",
    "\n",
    "---\n",
    "Make sure **Answer:** contains *only* your concise relative-position lines.\n",
    "\"\"\" % (\"\\n\".join(questions))\n",
    "\n",
    "    raw = query_model([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }], max_new_tokens=256)\n",
    "\n",
    "    if \"Answer:\" in raw:\n",
    "        answer_block = raw.split(\"Answer:\", 1)[1]\n",
    "    else:\n",
    "        answer_block = raw\n",
    "\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s+relative_to\\s+(\\w+)\\s*:\\s*(.+)$\", re.IGNORECASE)\n",
    "    rels = {}\n",
    "    for line in answer_block.splitlines():\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            a, b, ans = m.group(1).lower(), m.group(2).lower(), m.group(3).strip()\n",
    "            rels[(a, b)] = ans\n",
    "    return rels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = sorted(\n",
    "        f for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    )\n",
    "    if not files:\n",
    "        print(\"No images found in\", OUTPUT_DIR)\n",
    "        exit(1)\n",
    "\n",
    "    for fname in files:\n",
    "        img_path = os.path.join(OUTPUT_DIR, fname)\n",
    "        img = Image.open(img_path).convert(\"RGBA\")\n",
    "        print(f\"\\n=== {fname} ===\\n\")\n",
    "\n",
    "        # Step 1: List entities\n",
    "        entities = step1_list_entities(img)\n",
    "        print(\"Parsed entities:\", entities, \"\\n\")\n",
    "\n",
    "        # Step 2: Get relative positions\n",
    "        rels = step2_relative_positions_with_cot(img, entities)\n",
    "        if rels:\n",
    "            print(\"Relative positions:\")\n",
    "            for (a, b), txt in rels.items():\n",
    "                print(f\" - {a} relative to {b}: {txt}\")\n",
    "        else:\n",
    "            print(\"No relative positions extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llava model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 0) Exit any active venv\n",
    "deactivate 2>/dev/null || true\n",
    "\n",
    "# 1) Remove old LLaVA folder + venv\n",
    "rm -rf ~/LLaVA\n",
    "rm -rf ~/venvs/llava-env\n",
    "\n",
    "# 2) Clone LLaVA at the right tag\n",
    "git clone https://github.com/haotian-liu/LLaVA.git ~/LLaVA\n",
    "cd ~/LLaVA\n",
    "git checkout v1.2.2.post1\n",
    "\n",
    "# 3) Create & activate fresh venv\n",
    "python3 -m venv ~/venvs/llava-env\n",
    "source ~/venvs/llava-env/bin/activate\n",
    "\n",
    "# 4) Upgrade pip and install pinned deps + numpy<2 + protobuf\n",
    "pip install --upgrade pip\n",
    "pip install \"numpy<2\" \\\n",
    "  protobuf \\\n",
    "  accelerate==0.21.0 \\\n",
    "  einops==0.6.1 \\\n",
    "  sentencepiece==0.1.99 \\\n",
    "  timm==0.6.13 \\\n",
    "  tokenizers==0.15.1 \\\n",
    "  torch==2.1.2 \\\n",
    "  torchvision==0.16.2 \\\n",
    "  transformers==4.37.2\n",
    "\n",
    "# 5) Install LLaVA itself\n",
    "pip install -e .\n",
    "\n",
    "# 6) Silence tokenizer parallelism warning in future shells\n",
    "echo 'export TOKENIZERS_PARALLELISM=false' >> ~/.bashrc\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "# 7) Verify core imports\n",
    "pip show llava numpy protobuf\n",
    "python - << 'EOF'\n",
    "import llava, numpy as np, google.protobuf\n",
    "print(\"✅ LLaVA:\", llava.__file__)\n",
    "print(\"✅ NumPy:\", np.__version__)\n",
    "print(\"✅ Protobuf:\", google.protobuf.__version__)\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, LlamaTokenizer\n",
    "\n",
    "# Model name\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# Load the model in half-precision\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load processor and override with slow tokenizer\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "processor.tokenizer = LlamaTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Prepare input\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device, torch.float16)\n",
    "\n",
    "# Generate output\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "output = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove only the cached files for llava-1.5-7b-hf\n",
    "MODEL_CACHE_DIR=~/.cache/huggingface/transformers/llava-hf_llava-1.5-7b-hf*\n",
    "rm -rf $MODEL_CACHE_DIR\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
