{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yyyyifan/VLQA\") #[\"relation_KR_NC\"]\n",
    "\n",
    "sample = dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yyyyifan/VLQA\")\n",
    "\n",
    "test = dataset[\"test\"]\n",
    "\n",
    "questions = [sample[\"question\"] for sample in test]\n",
    "\n",
    "for i, q in enumerate(questions[:100]): \n",
    "    print(f\"{i+1}: {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yyyyifan/VLQA\")\n",
    "test = dataset[\"test\"]\n",
    "\n",
    "subset = test.select(range(20))  \n",
    "\n",
    "\n",
    "for i, sample in enumerate(subset):\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answer\"]  \n",
    "    print(f\"{i+1}: Q: {question}\\n   A: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "\n",
    "dataset = load_dataset(\"yyyyifan/VLQA\")\n",
    "sample = dataset[\"test\"][35]\n",
    "image = dataset[\"test\"][35][\"image\"]\n",
    "\n",
    "\n",
    "display(image)\n",
    "\n",
    "question = sample[\"question\"]\n",
    "answer = sample[\"answer\"]  \n",
    "print(f\"{i+1}: Q: {question}\\n   A: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from word2number import w2n\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "\n",
    "test = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "N = 50\n",
    "subset = test.select(range(N))\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def parse_prediction(pred_str):\n",
    "    m = re.search(r\"\\b(\\d+)\\b\", pred_str)\n",
    "    if m:\n",
    "        return int(m.group(1)), set()\n",
    "\n",
    "    try:\n",
    "        num = w2n.word_to_num(pred_str)\n",
    "        return num, set()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    quoted = re.findall(r'\"([^\"]+)\"', pred_str)\n",
    "    if quoted:\n",
    "        return None, {e.lower() for e in quoted}\n",
    "\n",
    "    ents = re.split(r\",|\\band\\b\", pred_str)\n",
    "    ents = {e.strip().lower() for e in ents if e.strip()}\n",
    "    return None, ents\n",
    "\n",
    "def get_prediction(image, question):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": question}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)    \n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    #print(input.keys -> mask)\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "# Counters\n",
    "howmany_total = 0\n",
    "howmany_correct = 0\n",
    "whichone_total = 0\n",
    "whichone_correct = 0\n",
    "\n",
    "results = []\n",
    "for i, sample in enumerate(subset):\n",
    "    image = sample[\"image\"]\n",
    "    question = sample[\"question\"]\n",
    "    gt = sample[\"answer\"]\n",
    "\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    display(image)\n",
    "    print(\"Question:\", question)\n",
    "\n",
    "    pred = get_prediction(image, question)\n",
    "    print(\"Prediction:\", pred)\n",
    "\n",
    "    is_howmany = question.lower().startswith(\"how many\")\n",
    "    is_whichone = question.lower().startswith(\"which one\")\n",
    "\n",
    "    if isinstance(gt, dict):\n",
    "        gt_number = gt.get(\"number\", None)\n",
    "        gt_entities = {e.lower() for e in gt.get(\"entities\", [])}\n",
    "    else:\n",
    "        gt_number, gt_entities = parse_prediction(gt)\n",
    "\n",
    "    pred_number, pred_entities = parse_prediction(pred)\n",
    "\n",
    "    if gt_number is not None:\n",
    "        correct = int(pred_number == gt_number)\n",
    "        score = correct\n",
    "        print(f\"Number: GT={gt_number}, Pred={pred_number} → {'+' if correct else '−'}\")\n",
    "\n",
    "        if is_howmany:\n",
    "            howmany_total += 1\n",
    "            howmany_correct += correct\n",
    "    elif gt_entities:\n",
    "        plus = pred_entities & gt_entities\n",
    "        minus = pred_entities - gt_entities\n",
    "        missing = gt_entities - pred_entities\n",
    "        score = len(plus) / len(gt_entities)\n",
    "        print(\"Entities:\")\n",
    "        print(\"  + correct :\", plus)\n",
    "        print(\"  − spurious:\", minus)\n",
    "        print(\"  ∅ missing :\", missing)\n",
    "\n",
    "        if is_whichone:\n",
    "            whichone_total += 1\n",
    "            whichone_correct += int(score > 0.5)\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i + 1,\n",
    "        \"question\": question,\n",
    "        \"gt_number\": gt_number,\n",
    "        \"gt_entities\": gt_entities,\n",
    "        \"pred\": pred,\n",
    "        \"pred_number\": pred_number,\n",
    "        \"pred_entities\": pred_entities,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "print(\"\\n\\n=== Ranked Performance (best → worst) ===\\n\")\n",
    "for r in results_sorted:\n",
    "    print(f\"Sample {r['index']}  SCORE={r['score']:.2f}\")\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    if r[\"gt_number\"] is not None:\n",
    "        print(f\"  GT number: {r['gt_number']}   Pred number: {r['pred_number']}\")\n",
    "    if r[\"gt_entities\"]:\n",
    "        print(f\"  GT entities: {r['gt_entities']}\")\n",
    "        print(f\"  Pred entities: {r['pred_entities']}\")\n",
    "    print(f\"  Raw prediction: {r['pred']}\\n\")\n",
    "\n",
    "print(\"\\n=== Category Accuracy Summary ===\")\n",
    "print(f\"How many:   {howmany_correct}/{howmany_total} correct ({howmany_correct / howmany_total:.0%})\" if howmany_total else \"No 'How many' questions.\")\n",
    "print(f\"Which one:  {whichone_correct}/{whichone_total} correct ({whichone_correct / whichone_total:.0%})\" if whichone_total else \"No 'Which one' questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"List all the entities in the image\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "    print(\"Prompt:\")\n",
    "    print(question)\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"How many arrows are there?\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"Which entities appear beside arrows?\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "\n",
    "test = load_dataset(\"yyyyifan/VLQA\", split=\"test\")                \n",
    "N = 50\n",
    "subset = test.select(range(N))                                   \n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")                                                                 \n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    quoted = re.findall(r'\"([^\"]+)\"', text)\n",
    "    if quoted:\n",
    "        return [e.strip().lower() for e in quoted]\n",
    "    parts = re.split(r\",|\\band\\b\", text)\n",
    "    return [p.strip().lower() for p in parts if p.strip()]\n",
    "\n",
    "def get_prediction(image, question):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": question}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "whichone_count = 0\n",
    "\n",
    "for i, sample in enumerate(subset, start=1):\n",
    "    question = sample[\"question\"]\n",
    "    if question.lower().startswith(\"how many\"):\n",
    "        continue                                               \n",
    "\n",
    "    if question.lower().startswith(\"which one\"):\n",
    "        whichone_count += 1\n",
    "        gt = sample[\"answer\"]\n",
    "        if isinstance(gt, dict):\n",
    "            ents = [e.lower() for e in gt.get(\"entities\", [])]\n",
    "        else:\n",
    "            ents = extract_entities(gt)\n",
    "        entity = ents[0] if ents else None\n",
    "\n",
    "        prompt = f'In which column is \"{entity}\". First second or third?'\n",
    "        print(f\"\\n--- Sample {i} (Which one) ---\")\n",
    "        display(sample[\"image\"])\n",
    "        print(\"Original Q:\", question)\n",
    "        print(\"New Prompt:\", prompt)\n",
    "\n",
    "        pred = get_prediction(sample[\"image\"], prompt)\n",
    "        print(\"Prediction\", pred)\n",
    "\n",
    "print(f\"\\nIssued {whichone_count} ″Which one″ prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "\n",
    "test = load_dataset(\"yyyyifan/VLQA\", split=\"test\")                \n",
    "N = 50\n",
    "subset = test.select(range(N))                                   \n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")                                                                 \n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    quoted = re.findall(r'\"([^\"]+)\"', text)\n",
    "    if quoted:\n",
    "        return [e.strip().lower() for e in quoted]\n",
    "    parts = re.split(r\",|\\band\\b\", text)\n",
    "    return [p.strip().lower() for p in parts if p.strip()]\n",
    "\n",
    "def get_prediction(image, question):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": question}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "whichone_count = 0\n",
    "\n",
    "for i, sample in enumerate(subset, start=1):\n",
    "    question = sample[\"question\"]\n",
    "    if question.lower().startswith(\"how many\"):\n",
    "        continue                                               \n",
    "\n",
    "    if question.lower().startswith(\"which one\"):\n",
    "        whichone_count += 1\n",
    "        gt = sample[\"answer\"]\n",
    "        if isinstance(gt, dict):\n",
    "            ents = [e.lower() for e in gt.get(\"entities\", [])]\n",
    "        else:\n",
    "            ents = extract_entities(gt)\n",
    "        entity = ents[0] if ents else None\n",
    "\n",
    "        prompt = f'What is the exact grid location (row, column) for \"{entity}\". Is it (first, first), (first, second), (first, third), (second, first), (second, second), (second, third), (third, first), (third, second), (third, third) ?'\n",
    "        print(f\"\\n--- Sample {i} (Which one) ---\")\n",
    "        display(sample[\"image\"])\n",
    "        print(\"Original Q:\", question)\n",
    "        print(\"New Prompt:\", prompt)\n",
    "\n",
    "        pred = get_prediction(sample[\"image\"], prompt)\n",
    "        print(\"Prediction\", pred)\n",
    "\n",
    "print(f\"\\nIssued {whichone_count} ″Which one″ prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "\n",
    "test = load_dataset(\"yyyyifan/VLQA\", split=\"test\")                \n",
    "N = 50\n",
    "subset = test.select(range(N))                                   \n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")                                                                 \n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    quoted = re.findall(r'\"([^\"]+)\"', text)\n",
    "    if quoted:\n",
    "        return [e.strip().lower() for e in quoted]\n",
    "    parts = re.split(r\",|\\band\\b\", text)\n",
    "    return [p.strip().lower() for p in parts if p.strip()]\n",
    "\n",
    "def get_prediction(image, question):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": question}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "whichone_count = 0\n",
    "\n",
    "for i, sample in enumerate(subset, start=1):\n",
    "    question = sample[\"question\"]\n",
    "    if question.lower().startswith(\"how many\"):\n",
    "        continue                                               \n",
    "\n",
    "    if question.lower().startswith(\"which one\"):\n",
    "        whichone_count += 1\n",
    "        gt = sample[\"answer\"]\n",
    "        if isinstance(gt, dict):\n",
    "            ents = [e.lower() for e in gt.get(\"entities\", [])]\n",
    "        else:\n",
    "            ents = extract_entities(gt)\n",
    "        entity = ents[0] if ents else None\n",
    "\n",
    "        prompt = f'In which row is \"{entity}\". First second or third?'\n",
    "        print(f\"\\n--- Sample {i} (Which one) ---\")\n",
    "        display(sample[\"image\"])\n",
    "        print(\"Original Q:\", question)\n",
    "        print(\"New Prompt:\", prompt)\n",
    "\n",
    "        pred = get_prediction(sample[\"image\"], prompt)\n",
    "        print(\"Prediction\", pred)\n",
    "\n",
    "print(f\"\\nIssued {whichone_count} ″Which one″ prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"How many entities are there in the left column?\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"How many entities are there in the first column?\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"How many arrows point upward vs. downward vs. diagonally?\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"How many entities are there?\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[\"test\"][idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "\n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
