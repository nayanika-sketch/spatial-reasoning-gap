{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {},
   "source": [
    "#current version error: after k == 14. I ahve to get teh exact image token split position and also change the input_ids potentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop analysis\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image, k):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"List all the entities in the image\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    print(inputs)\n",
    "    print(inputs.input_ids.shape)\n",
    "\n",
    "    \n",
    "    for i in range(k, 483):\n",
    "        inputs.attention_mask[0][i] = 0\n",
    "\n",
    "    print(inputs.attention_mask)\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "dataset   = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "\n",
    "#29\n",
    "\n",
    "K = 483\n",
    "for idx in range(K):\n",
    "    image = dataset[0][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "\n",
    "    k = idx\n",
    "    baseline_pred = get_prediction(image, k)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "  \n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "\n",
    "# 1) Load model, processor, tokenizer\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def get_prediction(image: Image.Image, k: int) -> str:\n",
    "    # 2) Build the “chat” prompt\n",
    "    prompt = \"List all the entities in the image\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # 3) Apply the chat template\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # 4) Tokenize + Preprocess\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 5) Locate the SEP (or EOS) token in input_ids\n",
    "    sep_token_id = tokenizer.sep_token_id if tokenizer.sep_token_id is not None else tokenizer.eos_token_id\n",
    "    # compare produces a BoolTensor\n",
    "    mask_positions = torch.nonzero(inputs.input_ids[0] == sep_token_id, as_tuple=False)\n",
    "    if mask_positions.numel() == 0:\n",
    "        raise RuntimeError(\"Could not find SEP/EOS token in input_ids!\")\n",
    "    sep_idx = mask_positions[0].item()\n",
    "\n",
    "    # 6) Hard-mask k tokens immediately after sep_idx\n",
    "    start = sep_idx + 1\n",
    "    end   = sep_idx + 1 + k\n",
    "    inputs.attention_mask[0, start:end] = 0\n",
    "\n",
    "    # (Optional) -- debug prints\n",
    "    print(f\"Masking tokens in positions [{start}:{end}) out of length {inputs.input_ids.shape[1]}\")\n",
    "\n",
    "    # 7) Generate\n",
    "    generated = model.generate(**inputs, max_new_tokens=128)\n",
    "    # 8) Trim off the prompt\n",
    "    trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated)\n",
    "    ]\n",
    "    # 9) Decode\n",
    "    return processor.batch_decode(\n",
    "        trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "\n",
    "# Load your VLQA test split\n",
    "dataset = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "image   = dataset[0][\"image\"]\n",
    "display(image)\n",
    "\n",
    "# Sweep k = 0 .. 482\n",
    "K = 483\n",
    "for k in range(K):\n",
    "    print(f\"\\n=== Masking {k} image tokens ===\")\n",
    "    pred = get_prediction(image, k)\n",
    "    print(\"→\", pred)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4",
   "metadata": {},
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ─── Model & Processor ────────────────────────────────────────────────────────\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # adjust these pixel constraints if needed\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# ─── Prediction Function ──────────────────────────────────────────────────────\n",
    "def get_prediction(image: Image.Image, k: int) -> str:\n",
    "    \"\"\"\n",
    "    Runs model inference on `image`, masking out the first `k` image patches.\n",
    "    Returns the generated text.\n",
    "    \"\"\"\n",
    "    prompt = \"List all the entities in the image\"\n",
    "\n",
    "    # 1) Build the chat-style messages\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # 2) Apply the chat template (inserts special tokens, etc.)\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 3) (internal) extract any extra vision/video info\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # 4) Tokenize + preprocess image\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    print(\"input_ids.shape:\",    inputs.input_ids.shape)\n",
    "    print(\"attention_mask.shape:\", inputs.attention_mask.shape)\n",
    "\n",
    "    # ── Dynamic masking ────────────────────────────────────────────────────────\n",
    "    input_ids = inputs.input_ids[0]         # (seq_len,)\n",
    "    mask      = inputs.attention_mask[0]    # (seq_len,)\n",
    "\n",
    "    # a) get the image-start token ID from the model config:\n",
    "    sep_token_id = model.config.image_token_id   # defaults to 151655\n",
    "\n",
    "    # b) find *all* positions where it appears, then pick the first one\n",
    "    sep_positions = (input_ids == sep_token_id).nonzero(as_tuple=True)[0]\n",
    "    if sep_positions.numel() == 0:\n",
    "        raise ValueError(f\"Image-start token id {sep_token_id} not found in input_ids!\")\n",
    "    sep_idx = sep_positions[0].item()\n",
    "\n",
    "    # c) how many image patches follow?\n",
    "    seq_len       = input_ids.shape[-1]\n",
    "    num_image_tok = seq_len - sep_idx - 1\n",
    "\n",
    "    # d) clamp k so we never go out of bounds\n",
    "    k_clamped = min(k, num_image_tok)\n",
    "\n",
    "    # e) zero‐out exactly the first k_clamped image‐patch positions\n",
    "    start = sep_idx + 1\n",
    "    end   = sep_idx + 1 + k_clamped\n",
    "    mask[start:end] = 0\n",
    "\n",
    "    # f) write it back\n",
    "    inputs.attention_mask = mask.unsqueeze(0)\n",
    "\n",
    "    print(f\"Masking out image tokens {start}–{end-1}  (k={k_clamped})\")\n",
    "\n",
    "    # 5) Generate\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # 6) Strip off the input portion and decode\n",
    "    trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output = processor.batch_decode(\n",
    "        trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "\n",
    "    return output\n",
    "\n",
    "# ─── Main Loop over Dataset ──────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "\n",
    "    # try varying k from 0 (no masking) up to some small number\n",
    "    for k in range(0, 100):\n",
    "        print(f\"\\n\\n=== Mask k = {k} ===\\n\")\n",
    "        image = dataset[0][\"image\"]\n",
    "        display(image)\n",
    "\n",
    "        prediction = get_prediction(image, k)\n",
    "        print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5",
   "metadata": {},
   "source": [
    "#current trial\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import difflib\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from IPython.display import display\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    # Parameters for pixel constraint, adjust as needed:\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction(image):\n",
    "    \"\"\"\n",
    "    Runs model inference given an image.\n",
    "    \"\"\"\n",
    "    prompt = \"List all the entities in the image\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "   \n",
    "\n",
    "results = []  \n",
    "dataset   = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "\n",
    "for idx in range(10):\n",
    "    image = dataset[idx][\"image\"]\n",
    "\n",
    "    print(\"Sample Image:\")\n",
    "    display(image)\n",
    "    \n",
    "    baseline_pred = get_prediction(image)\n",
    "\n",
    "    print(\"Baseline prediction:\")\n",
    "    print(baseline_pred)\n",
    "  \n",
    "\n",
    "    display_images = [image]\n",
    "    display_titles = [\"Original\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6",
   "metadata": {},
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "\n",
    "dataset   = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "# 7) Prediction with patch_mask\n",
    "def predict_with_patch_mask(pixel_values, patch_mask, prompt: str):\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[\n",
    "            {\"type\":\"image\",\"image\":image},\n",
    "            {\"type\":\"text\",\"text\":prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text], images=image, return_tensors=\"pt\", padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    print(f\"inputs: {inputs.input_ids}\")\n",
    "    print(f\"attention_mask: {inputs.attention_mask}\")\n",
    "    print(f\"pixel_values: {inputs.pixel_values}\")\n",
    "    print(f\"attention_mask shape: {inputs.attention_mask.shape}\")\n",
    "    print(f\"inputs shape: {inputs.input_ids.shape}\")\n",
    "\n",
    "    \n",
    "    for i in range(k, 483):\n",
    "        inputs.attention_mask[0][i] = 0\n",
    "    print(inputs.attention_mask)\n",
    "\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, gen)\n",
    "    ]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "prompt = \"List all the entities in the image?\"\n",
    "num_patches = 483\n",
    "for k in range(1, num_patches + 1):\n",
    "    image = dataset[0][\"image\"]\n",
    "    output = predict_with_patch_mask(image, k, prompt)\n",
    "    print(f\"→ Prediction: {output}\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7",
   "metadata": {},
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 1) Load model & processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# 2) Load & resize image\n",
    "ds   = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "orig = ds[0][\"image\"].convert(\"RGB\")\n",
    "# Derive patch grid from config\n",
    "vcfg        = model.config.vision_config\n",
    "px, ms      = vcfg.patch_size, vcfg.spatial_merge_size\n",
    "patch_size  = px * ms                # e.g. 28\n",
    "W, H        = orig.size\n",
    "new_W       = (W // patch_size) * patch_size\n",
    "new_H       = (H // patch_size) * patch_size\n",
    "orig        = orig.resize((new_W, new_H), Image.BILINEAR)\n",
    "\n",
    "# 3) One-time vision preprocessing -> sequence of patch embeddings\n",
    "feat           = processor.image_processor(images=orig, return_tensors=\"pt\")\n",
    "pixel_values   = feat.pixel_values.to(model.device)  # shape [1, num_patches, embed_dim]\n",
    "\n",
    "# 4) Compute grid dims\n",
    "patches_w      = new_W // patch_size\n",
    "patches_h      = new_H // patch_size\n",
    "num_patches    = patches_w * patches_h\n",
    "\n",
    "# 5) Helper to build a patch_mask of length num_patches\n",
    "def make_patch_mask(k: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Returns a boolean mask of shape [1, num_patches] where the first k\n",
    "    entries are True (visible) and the rest False.\n",
    "    \"\"\"\n",
    "    mask = torch.zeros((1, num_patches), dtype=torch.bool, device=device)\n",
    "    mask[0, :k] = True\n",
    "    return mask\n",
    "\n",
    "# 6) Prediction helper injecting patch_mask\n",
    "def predict_with_patch_mask(pixel_values, patch_mask, prompt):\n",
    "    # Build dummy chat inputs\n",
    "    dummy = Image.new(\"RGB\", (new_W, new_H))\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[\n",
    "            {\"type\":\"image\",\"image\":dummy},\n",
    "            {\"type\":\"text\",\"text\":prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[text], images=dummy, return_tensors=\"pt\", padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Overwrite with our precomputed values & mask\n",
    "    inputs.pixel_values = pixel_values\n",
    "    inputs.patch_mask   = patch_mask\n",
    "\n",
    "    # Generate\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    # Strip prompt tokens\n",
    "    trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, gen)\n",
    "    ]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "# 7) Iterate revealing 1 → num_patches consecutively\n",
    "prompt = \"List all the entities in the image?\"\n",
    "for k in range(1, num_patches + 1):\n",
    "    pmask = make_patch_mask(k, model.device)\n",
    "    out   = predict_with_patch_mask(pixel_values, pmask, prompt)\n",
    "    print(f\"→ {k}/{num_patches} patches visible → {out}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8",
   "metadata": {},
   "source": [
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load model & processor\n",
    "# -----------------------------------------------------------------------------\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load & resize one test image\n",
    "# -----------------------------------------------------------------------------\n",
    "ds = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "orig: Image.Image = ds[0][\"image\"].convert(\"RGB\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Derive effective patch parameters from vision config\n",
    "# -----------------------------------------------------------------------------\n",
    "# Get parameters from vision configuration\n",
    "vision_config = model.config.vision_config\n",
    "spatial_patch = vision_config.patch_size          # 14\n",
    "merge_size = vision_config.spatial_merge_size     # 2\n",
    "patch_size = spatial_patch * merge_size           # 14*2 = 28\n",
    "\n",
    "# Calculate image size as multiple of effective patch size\n",
    "image_size = patch_size * 8  # 28*8 = 224\n",
    "orig = orig.resize((image_size, image_size), Image.BILINEAR)\n",
    "print(f\"Resized image to {image_size}×{image_size} (effective patch={patch_size})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Process image and extract features\n",
    "# -----------------------------------------------------------------------------\n",
    "feat = processor.image_processor(images=orig, return_tensors=\"pt\")\n",
    "pixel_values = feat.pixel_values.to(model.device)  # [1, C, H, W]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Calculate grid dimensions\n",
    "# -----------------------------------------------------------------------------\n",
    "assert image_size % patch_size == 0\n",
    "patches_per_side = image_size // patch_size\n",
    "num_patches = patches_per_side ** 2\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Vectorized mask creation\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_patch_mask(k: int, device: torch.device) -> torch.Tensor:\n",
    "    mask = torch.zeros_like(pixel_values)\n",
    "    idxs = torch.randperm(num_patches, device=device)[:k]\n",
    "    \n",
    "    # Calculate grid positions for selected patches\n",
    "    rows = idxs // patches_per_side\n",
    "    cols = idxs % patches_per_side\n",
    "    \n",
    "    # Create mask using broadcasting\n",
    "    y_start = rows * patch_size\n",
    "    y_end = y_start + patch_size\n",
    "    x_start = cols * patch_size\n",
    "    x_end = x_start + patch_size\n",
    "    \n",
    "    # Vectorized masking\n",
    "    mask[..., y_start[:, None, None], x_start[None, :]] = 1.0\n",
    "    mask[..., y_end[:, None, None], x_end[None, :]] = 0.0\n",
    "    return mask.bool()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Prediction helper with proper mask handling\n",
    "# -----------------------------------------------------------------------------\n",
    "def predict_with_mask(masked_pixels: torch.Tensor, prompt: str) -> str:\n",
    "    # Create dummy image for processor (maintains processing pipeline)\n",
    "    dummy = Image.new(\"RGB\", (image_size, image_size))\n",
    "    \n",
    "    # Build message template\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": dummy},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "    \n",
    "    # Process inputs with proper mask integration\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[text], \n",
    "        images=dummy, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Directly replace pixel values with masked version\n",
    "    inputs[\"pixel_values\"] = masked_pixels\n",
    "    \n",
    "    # Generate with modified inputs\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    \n",
    "    # Post-process output\n",
    "    trimmed = gen[:, inputs.input_ids.shape[1]:]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Progressive masking with improved efficiency\n",
    "# -----------------------------------------------------------------------------\n",
    "prompt = \"List all the entities in the image?\"\n",
    "for k in range(1, min(num_patches, 1000) + 1):\n",
    "    mask = create_patch_mask(k, model.device)\n",
    "    masked_input = pixel_values * mask\n",
    "    output = predict_with_mask(masked_input, prompt)\n",
    "    print(f\"→ {k:3d}/{num_patches} patches visible → {output}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9",
   "metadata": {},
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTForImageClassification, AutoImageProcessor\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load model and processor\n",
    "# -----------------------------------------------------------------------------\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load and preprocess image\n",
    "# -----------------------------------------------------------------------------\n",
    "ds = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "orig = ds[0][\"image\"]\n",
    "inputs = processor(images=orig, return_tensors=\"pt\")\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Get model configuration parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "patch_size = model.config.patch_size\n",
    "image_size = model.config.image_size\n",
    "num_channels = model.config.num_channels\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Calculate patch grid dimensions\n",
    "# -----------------------------------------------------------------------------\n",
    "assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "patches_per_side = image_size // patch_size\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Enhanced masking function using tensor operations\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_patch_mask(k: int, device: torch.device) -> torch.Tensor:\n",
    "    # Create base mask with all patches masked\n",
    "    mask = torch.zeros((1, num_channels, image_size, image_size), device=device)\n",
    "    \n",
    "    # Calculate visible patches' positions\n",
    "    rows = torch.arange(k) // patches_per_side\n",
    "    cols = torch.arange(k) % patches_per_side\n",
    "    \n",
    "    # Vectorized mask update\n",
    "    for row, col in zip(rows, cols):\n",
    "        h_start = row * patch_size\n",
    "        w_start = col * patch_size\n",
    "        mask[:, :, h_start:h_start+patch_size, w_start:w_start+patch_size] = 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Perform progressive prediction with increasing visible patches\n",
    "# -----------------------------------------------------------------------------\n",
    "device = model.device\n",
    "pixel_values = pixel_values.to(device)\n",
    "\n",
    "for k in range(1, min(num_patches, 1000) + 1):\n",
    "    # Create and apply mask\n",
    "    mask = create_patch_mask(k, device)\n",
    "    masked_input = pixel_values * mask\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_input)\n",
    "    \n",
    "    # Interpret results\n",
    "    logits = outputs.logits\n",
    "    predicted_class = model.config.id2label[logits.argmax(-1).item()]\n",
    "    \n",
    "    print(f\"→ Visible Patches {k:3d}/{num_patches}: Prediction: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10",
   "metadata": {},
   "source": [
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def get_prediction(image: Image.Image):\n",
    "    prompt = \"How many entities are there?\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    gen_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen_ids)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "dataset = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "orig = dataset[0][\"image\"]  \n",
    "patch_size = 14  \n",
    "W, H = orig.size\n",
    "cols = W // patch_size\n",
    "rows = H // patch_size\n",
    "total_patches = cols * rows\n",
    "\n",
    "print(f\"Image size: {W}×{H}, grid: {cols}×{rows} = {total_patches} patches\\n\")\n",
    "\n",
    "for k in range(1, min(total_patches, 1000) + 1):  \n",
    "    mask = Image.new(\"RGB\", (W, H), color=(0, 0, 0))\n",
    "    for idx in range(k):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        left = col * patch_size\n",
    "        upper = row * patch_size\n",
    "        box = (left, upper, left + patch_size, upper + patch_size)\n",
    "        patch = orig.crop(box)             \n",
    "        mask.paste(patch, box)            \n",
    "\n",
    "    print(f\"\\n=== Using first {k} patch(es) ===\")\n",
    "    display(mask)\n",
    "\n",
    "    out = get_prediction(mask)\n",
    "    print(\"Prediction:\", out)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11",
   "metadata": {},
   "source": [
    "#good ones"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12",
   "metadata": {},
   "source": [
    "#trial\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load model & processor\n",
    "# -----------------------------------------------------------------------------\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load & resize base image\n",
    "# -----------------------------------------------------------------------------\n",
    "ds   = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "orig = ds[0][\"image\"].convert(\"RGB\")\n",
    "\n",
    "vcfg       = model.config.vision_config\n",
    "patch_sz   = vcfg.patch_size * vcfg.spatial_merge_size\n",
    "\n",
    "W, H       = orig.size\n",
    "new_W      = (W // patch_sz) * patch_sz\n",
    "new_H      = (H // patch_sz) * patch_sz\n",
    "orig       = orig.resize((new_W, new_H), Image.BILINEAR)\n",
    "\n",
    "patches_w  = new_W // patch_sz\n",
    "patches_h  = new_H // patch_sz\n",
    "num_patches = patches_w * patches_h\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) One helper to build a boolean mask for visualization\n",
    "# -----------------------------------------------------------------------------\n",
    "def make_visual(k):\n",
    "    vis = Image.new(\"RGB\", (new_W, new_H))\n",
    "    for idx in range(k):\n",
    "        r, c = divmod(idx, patches_w)\n",
    "        box = (c*patch_sz, r*patch_sz, (c+1)*patch_sz, (r+1)*patch_sz)\n",
    "        vis.paste(orig.crop(box), box)\n",
    "    return vis\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Combined processor + generate helper\n",
    "# -----------------------------------------------------------------------------\n",
    "def predict_with_vis(vis_img: Image.Image, prompt: str):\n",
    "    # a) Build the chat prompt with the *masked* image\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": vis_img},\n",
    "            {\"type\": \"text\",  \"text\":  prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # b) PROCESS (once) — this will give us both pixel_values *and* patch_mask\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[vis_img],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(model.device)  # internally sets inputs.pixel_values & inputs.patch_mask :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "    # c) GENERATE\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    # trim off the prompt tokens\n",
    "    gen = out[:, inputs.input_ids.shape[-1]:]\n",
    "    return processor.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Reveal‐and‐poll loop\n",
    "# -----------------------------------------------------------------------------\n",
    "prompt = \"List all the entities in the image?\"\n",
    "\n",
    "for k in range(1, num_patches + 1):\n",
    "    vis = make_visual(k)\n",
    "\n",
    "    # show the current mask\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(vis)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"{k}/{num_patches} patches visible\")\n",
    "    plt.show()\n",
    "\n",
    "    # call the combined helper\n",
    "    answer = predict_with_vis(vis, prompt)\n",
    "    print(f\"→ Prediction ({k} patches): {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13",
   "metadata": {},
   "source": [
    "#trial\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 1) Load model & processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# 2) Load & resize image\n",
    "ds   = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "orig = ds[0][\"image\"].convert(\"RGB\")\n",
    "# Derive patch grid from config\n",
    "vcfg        = model.config.vision_config\n",
    "px, ms      = vcfg.patch_size, vcfg.spatial_merge_size\n",
    "patch_size  = px * ms                # e.g. 28\n",
    "W, H        = orig.size\n",
    "new_W       = (W // patch_size) * patch_size\n",
    "new_H       = (H // patch_size) * patch_size\n",
    "orig        = orig.resize((new_W, new_H), Image.BILINEAR)\n",
    "\n",
    "# 3) One-time vision preprocessing -> sequence of patch embeddings\n",
    "feat           = processor.image_processor(images=orig, return_tensors=\"pt\")\n",
    "pixel_values   = feat.pixel_values.to(model.device)  # shape [1, num_patches, embed_dim]\n",
    "\n",
    "# 4) Compute grid dims\n",
    "patches_w      = new_W // patch_size\n",
    "patches_h      = new_H // patch_size\n",
    "num_patches    = patches_w * patches_h\n",
    "\n",
    "# 5) Helper to build a patch_mask of length num_patches\n",
    "def make_patch_mask(k: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Returns a boolean mask of shape [1, num_patches] where the first k\n",
    "    entries are True (visible) and the rest False.\n",
    "    \"\"\"\n",
    "    mask = torch.zeros((1, num_patches), dtype=torch.bool, device=device)\n",
    "    mask[0, :k] = True\n",
    "    return mask\n",
    "\n",
    "# 6) Prediction helper injecting patch_mask\n",
    "def predict_with_patch_mask(pixel_values, patch_mask, prompt):\n",
    "    # Build dummy chat inputs\n",
    "    dummy = Image.new(\"RGB\", (new_W, new_H))\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[\n",
    "            {\"type\":\"image\",\"image\":dummy},\n",
    "            {\"type\":\"text\",\"text\":prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[text], images=dummy, return_tensors=\"pt\", padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Overwrite with our precomputed values & mask\n",
    "    inputs.pixel_values = pixel_values\n",
    "    inputs.patch_mask   = patch_mask\n",
    "\n",
    "    # Generate\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    # Strip prompt tokens\n",
    "    trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, gen)\n",
    "    ]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "# 7) Iterate revealing 1 → num_patches consecutively\n",
    "prompt = \"List all the entities in the image?\"\n",
    "for k in range(1, num_patches + 1):\n",
    "    pmask = make_patch_mask(k, model.device)\n",
    "    out   = predict_with_patch_mask(pixel_values, pmask, prompt)\n",
    "    print(f\"→ {k}/{num_patches} patches visible → {out}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14",
   "metadata": {},
   "source": [
    "#trial\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load model & processor\n",
    "# -----------------------------------------------------------------------------\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load & resize one test image\n",
    "# -----------------------------------------------------------------------------\n",
    "ds = load_dataset(\"yyyyifan/VLQA\", split=\"test\")\n",
    "orig: Image.Image = ds[0][\"image\"].convert(\"RGB\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Derive effective patch parameters from vision config\n",
    "# -----------------------------------------------------------------------------\n",
    "# Get parameters from vision configuration\n",
    "vision_config = model.config.vision_config\n",
    "spatial_patch = vision_config.patch_size          # 14\n",
    "merge_size = vision_config.spatial_merge_size     # 2\n",
    "patch_size = spatial_patch * merge_size           # 14*2 = 28\n",
    "\n",
    "# Calculate image size as multiple of effective patch size\n",
    "image_size = patch_size * 8  # 28*8 = 224\n",
    "orig = orig.resize((image_size, image_size), Image.BILINEAR)\n",
    "print(f\"Resized image to {image_size}×{image_size} (effective patch={patch_size})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Process image and extract features\n",
    "# -----------------------------------------------------------------------------\n",
    "feat = processor.image_processor(images=orig, return_tensors=\"pt\")\n",
    "pixel_values = feat.pixel_values.to(model.device)  # [1, C, H, W]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Calculate grid dimensions\n",
    "# -----------------------------------------------------------------------------\n",
    "assert image_size % patch_size == 0\n",
    "patches_per_side = image_size // patch_size\n",
    "num_patches = patches_per_side ** 2\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Vectorized mask creation\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_patch_mask(k: int, device: torch.device) -> torch.Tensor:\n",
    "    mask = torch.zeros_like(pixel_values)\n",
    "    idxs = torch.randperm(num_patches, device=device)[:k]\n",
    "    \n",
    "    # Calculate grid positions for selected patches\n",
    "    rows = idxs // patches_per_side\n",
    "    cols = idxs % patches_per_side\n",
    "    \n",
    "    # Create mask using broadcasting\n",
    "    y_start = rows * patch_size\n",
    "    y_end = y_start + patch_size\n",
    "    x_start = cols * patch_size\n",
    "    x_end = x_start + patch_size\n",
    "    \n",
    "    # Vectorized masking\n",
    "    mask[..., y_start[:, None, None], x_start[None, :]] = 1.0\n",
    "    mask[..., y_end[:, None, None], x_end[None, :]] = 0.0\n",
    "    return mask.bool()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Prediction helper with proper mask handling\n",
    "# -----------------------------------------------------------------------------\n",
    "def predict_with_mask(masked_pixels: torch.Tensor, prompt: str) -> str:\n",
    "    # Create dummy image for processor (maintains processing pipeline)\n",
    "    dummy = Image.new(\"RGB\", (image_size, image_size))\n",
    "    \n",
    "    # Build message template\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": dummy},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "    \n",
    "    # Process inputs with proper mask integration\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[text], \n",
    "        images=dummy, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Directly replace pixel values with masked version\n",
    "    inputs[\"pixel_values\"] = masked_pixels\n",
    "    \n",
    "    # Generate with modified inputs\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    \n",
    "    # Post-process output\n",
    "    trimmed = gen[:, inputs.input_ids.shape[1]:]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Progressive masking with improved efficiency\n",
    "# -----------------------------------------------------------------------------\n",
    "prompt = \"List all the entities in the image?\"\n",
    "for k in range(1, min(num_patches, 1000) + 1):\n",
    "    mask = create_patch_mask(k, model.device)\n",
    "    masked_input = pixel_values * mask\n",
    "    output = predict_with_mask(masked_input, prompt)\n",
    "    print(f\"→ {k:3d}/{num_patches} patches visible → {output}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15",
   "metadata": {},
   "source": [
    "#what worked previous iteration (input masking)\n",
    "#good cropping patch wise from the input\n",
    "\n",
    "import math                                             \n",
    "import torch\n",
    "from PIL import Image                                   \n",
    "import matplotlib.pyplot as plt                         \n",
    "from datasets import load_dataset                       \n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 1) Load one test image\n",
    "ds = load_dataset(\"yyyyifan/VLQA\", split=\"test\")         \n",
    "orig: Image.Image = ds[0][\"image\"]                       \n",
    "W, H = orig.size\n",
    "print(f\"Original image size: {W}×{H}\")\n",
    "\n",
    "patch_unit, merge_size = 14, 2\n",
    "P = patch_unit * merge_size                             \n",
    "cols = W // P\n",
    "total = cols * (H // P)\n",
    "print(f\"Grid: {cols} columns, total {total} patches of size {P}×{P}\")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")                                                        \n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def predict(img: Image.Image, prompt=\"How many arrows are there?\"):\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[{\"type\":\"image\",\"image\":img},{\"type\":\"text\",\"text\":prompt}]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=img, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "K = 1000\n",
    "for k in range(1, min(total, K) + 1):\n",
    "    rows = math.ceil(k / cols)\n",
    "    w_mosaic = min(k, cols) * P\n",
    "    h_mosaic = rows * P\n",
    "\n",
    "    mosaic = Image.new(\"RGB\", (w_mosaic, h_mosaic))\n",
    "\n",
    "    for idx in range(k):\n",
    "        r, c = divmod(idx, cols)\n",
    "        src_box = (c*P, r*P, c*P+P, r*P+P)\n",
    "        patch = orig.crop(src_box)                        \n",
    "        dst = ( (idx % cols)*P, (idx // cols)*P )\n",
    "        mosaic.paste(patch, (dst[0], dst[1], dst[0]+P, dst[1]+P))  \n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(mosaic)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Mosaic of first {k} patches\")\n",
    "    plt.show()                                         \n",
    "\n",
    "    out = predict(mosaic)\n",
    "    print(f\"Prediction with {k} patches:\", out, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16",
   "metadata": {},
   "source": [
    "#what worked previous iteration (input masking)\n",
    "#good cropping patch wise from the input\n",
    "\n",
    "import math                                             \n",
    "import torch\n",
    "from PIL import Image                                   \n",
    "import matplotlib.pyplot as plt                         \n",
    "from datasets import load_dataset                       \n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 1) Load one test image\n",
    "ds = load_dataset(\"yyyyifan/VLQA\", split=\"test\")         \n",
    "orig: Image.Image = ds[0][\"image\"]                       \n",
    "W, H = orig.size\n",
    "print(f\"Original image size: {W}×{H}\")\n",
    "\n",
    "patch_unit, merge_size = 14, 2\n",
    "P = patch_unit * merge_size                             \n",
    "cols = W // P\n",
    "total = cols * (H // P)\n",
    "print(f\"Grid: {cols} columns, total {total} patches of size {P}×{P}\")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")                                                        \n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def predict(img: Image.Image, prompt):\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[{\"type\":\"image\",\"image\":img},{\"type\":\"text\",\"text\":prompt}]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=img, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "K = 1000\n",
    "questions = [\n",
    "    # Absolute positioning\n",
    "    \"Which entity sits in the top-left corner of the image?\",\n",
    "    \"Which entity lies exactly on the horizontal center line?\",\n",
    "    \"What label is closest to the bottom edge of the image?\",\n",
    "    \"Which entity is directly to the right of the left dashed vertical line?\",\n",
    "    \"Which entities lie in the left half vs. the right half of the diagram?\",\n",
    "\n",
    "    # Relative positioning\n",
    "    \"Which entity is immediately above 'carton'?\",\n",
    "    \"Which entity is immediately below the label 'pot'?\",\n",
    "    \"Which label is directly to the left of 'slipper'?\",\n",
    "    \"Which label is directly to the right of 'seashell'?\",\n",
    "    \"Which two entities are vertically aligned (share the same x-coordinate)?\",\n",
    "\n",
    "    # Directional inference (arrow orientation)\n",
    "    \"How many arrows point upward?\",\n",
    "    \"How many arrows point downward?\",\n",
    "    \"How many arrows point left?\",\n",
    "    \"How many arrows point right?\",\n",
    "    \"Which arrow has the steepest slope? Which has the shallowest?\",\n",
    "\n",
    "    # Connectivity & adjacency\n",
    "    \"Which entities are directly connected to 'seashell' by an arrow?\",\n",
    "    \"List all entities that have an incoming arrow (arrowhead touching them).\",\n",
    "    \"List all entities that have an outgoing arrow (tail touching them).\",\n",
    "    \"Which entity has the highest number of total connections (in+out)?\",\n",
    "    \"Are there any entities not connected by any arrow?\",\n",
    "\n",
    "    # Path‐finding and reachability\n",
    "    \"Is there a path of arrows leading from 'seashell' to 'pot'?\",\n",
    "    \"If you follow arrows as directed edges, which entities can you reach starting at 'carton'?\",\n",
    "    \"Are there any cycles (loops) if you follow arrow directions?\",\n",
    "\n",
    "    # Intersection & occlusion\n",
    "    \"How many pairs of arrows cross each other?\",\n",
    "    \"At roughly what coordinates does the single intersection occur?\",\n",
    "    \"Which arrow crosses over the longest horizontal span?\",\n",
    "\n",
    "    # Distance & spacing\n",
    "    \"Which two entities are the farthest apart (Euclidean distance on the diagram)?\",\n",
    "    \"Which two entities are the closest together?\",\n",
    "    \"Which row (horizontal strip between dashed lines) contains the greatest number of entity labels?\",\n",
    "\n",
    "    # Alignment & grouping\n",
    "    \"Are any three entities collinear? If so, which ones?\",\n",
    "    \"Do any four entities form the corners of a rectangle?\",\n",
    "    \"Which entities lie on the same imaginary diagonal line (↘ or ↗)?\",\n",
    "\n",
    "    # Composite spatial reasoning\n",
    "    \"If you partition the canvas into quadrants by the dashed center lines, how many entities fall in each quadrant?\",\n",
    "    \"Which arrow spans more than one quadrant?\",\n",
    "    \"Which quadrant contains the highest arrow density?\"\n",
    "]\n",
    "for prompt in questions: \n",
    "    for k in range(1, min(total, K) + 1):\n",
    "        rows = math.ceil(k / cols)\n",
    "        w_mosaic = min(k, cols) * P\n",
    "        h_mosaic = rows * P\n",
    "    \n",
    "        mosaic = Image.new(\"RGB\", (w_mosaic, h_mosaic))\n",
    "    \n",
    "        for idx in range(k):\n",
    "            r, c = divmod(idx, cols)\n",
    "            src_box = (c*P, r*P, c*P+P, r*P+P)\n",
    "            patch = orig.crop(src_box)                        \n",
    "            dst = ( (idx % cols)*P, (idx // cols)*P )\n",
    "            mosaic.paste(patch, (dst[0], dst[1], dst[0]+P, dst[1]+P))  \n",
    "    \n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(mosaic)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Mosaic of first {k} patches\")\n",
    "        plt.show()                                         \n",
    "    \n",
    "        out = predict(mosaic, prompt)\n",
    "        print(f\"Prediction with {k} patches:\", out, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17",
   "metadata": {},
   "source": [
    "#what worked previous iteration (input masking)\n",
    "#good cropping patch wise from the input\n",
    "\n",
    "import math                                             \n",
    "import torch\n",
    "from PIL import Image                                   \n",
    "import matplotlib.pyplot as plt                         \n",
    "from datasets import load_dataset                       \n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 1) Load one test image\n",
    "ds = load_dataset(\"yyyyifan/VLQA\", split=\"test\")         \n",
    "orig: Image.Image = ds[0][\"image\"]                       \n",
    "W, H = orig.size\n",
    "print(f\"Original image size: {W}×{H}\")\n",
    "\n",
    "patch_unit, merge_size = 14, 2\n",
    "P = patch_unit * merge_size                             \n",
    "cols = W // P\n",
    "total = cols * (H // P)\n",
    "print(f\"Grid: {cols} columns, total {total} patches of size {P}×{P}\")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")                                                        \n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "def predict(img: Image.Image, prompt):\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[{\"type\":\"image\",\"image\":img},{\"type\":\"text\",\"text\":prompt}]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=img, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    gen = model.generate(**inputs, max_new_tokens=64)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "K = 1000\n",
    "questions = [\n",
    "    # Relative positioning\n",
    "    \"Which entity is immediately below the label 'pot'?\",\n",
    "    \"Which label is directly to the left of 'slipper'?\",\n",
    "    \"Which label is directly to the right of 'seashell'?\",\n",
    "    \"Which two entities are vertically aligned (share the same x-coordinate)?\",\n",
    "\n",
    "    # Directional inference (arrow orientation)\n",
    "    \"How many arrows point upward?\",\n",
    "    \"How many arrows point downward?\",\n",
    "    \"How many arrows point left?\",\n",
    "    \"How many arrows point right?\",\n",
    "    \"Which arrow has the steepest slope? Which has the shallowest?\",\n",
    "\n",
    "    # Connectivity & adjacency\n",
    "    \"Which entities are directly connected to 'seashell' by an arrow?\",\n",
    "    \"List all entities that have an incoming arrow (arrowhead touching them).\",\n",
    "    \"List all entities that have an outgoing arrow (tail touching them).\",\n",
    "    \"Which entity has the highest number of total connections (in+out)?\",\n",
    "    \"Are there any entities not connected by any arrow?\",\n",
    "\n",
    "    # Path‐finding and reachability\n",
    "    \"Is there a path of arrows leading from 'seashell' to 'pot'?\",\n",
    "    \"If you follow arrows as directed edges, which entities can you reach starting at 'carton'?\",\n",
    "    \"Are there any cycles (loops) if you follow arrow directions?\",\n",
    "\n",
    "    # Intersection & occlusion\n",
    "    \"How many pairs of arrows cross each other?\",\n",
    "    \"At roughly what coordinates does the single intersection occur?\",\n",
    "    \"Which arrow crosses over the longest horizontal span?\",\n",
    "\n",
    "    # Distance & spacing\n",
    "    \"Which two entities are the farthest apart (Euclidean distance on the diagram)?\",\n",
    "    \"Which two entities are the closest together?\",\n",
    "    \"Which row (horizontal strip between dashed lines) contains the greatest number of entity labels?\",\n",
    "\n",
    "    # Alignment & grouping\n",
    "    \"Are any three entities collinear? If so, which ones?\",\n",
    "    \"Do any four entities form the corners of a rectangle?\",\n",
    "    \"Which entities lie on the same imaginary diagonal line (↘ or ↗)?\",\n",
    "\n",
    "    # Composite spatial reasoning\n",
    "    \"If you partition the canvas into quadrants by the dashed center lines, how many entities fall in each quadrant?\",\n",
    "    \"Which arrow spans more than one quadrant?\",\n",
    "    \"Which quadrant contains the highest arrow density?\"\n",
    "]\n",
    "for prompt in questions: \n",
    "    for k in range(1, min(total, K) + 1):\n",
    "        rows = math.ceil(k / cols)\n",
    "        w_mosaic = min(k, cols) * P\n",
    "        h_mosaic = rows * P\n",
    "    \n",
    "        mosaic = Image.new(\"RGB\", (w_mosaic, h_mosaic))\n",
    "    \n",
    "        for idx in range(k):\n",
    "            r, c = divmod(idx, cols)\n",
    "            src_box = (c*P, r*P, c*P+P, r*P+P)\n",
    "            patch = orig.crop(src_box)                        \n",
    "            dst = ( (idx % cols)*P, (idx // cols)*P )\n",
    "            mosaic.paste(patch, (dst[0], dst[1], dst[0]+P, dst[1]+P))  \n",
    "    \n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(mosaic)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Mosaic of first {k} patches\")\n",
    "        plt.show()                                         \n",
    "    \n",
    "        out = predict(mosaic, prompt)\n",
    "        print(f\"Prediction with {k} patches:\", out, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
